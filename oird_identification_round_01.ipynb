{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SnorkelMED - Identifying Opioid-Induced Respiratory Depression  \n",
    "\n",
    "The purpose of this analysis is to probabilistically identify which patient visits included an opioid-induced respiratory depression (OIRD) event. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from snorkel.labeling import labeling_function, PandasLFApplier, LFAnalysis\n",
    "from snorkel.labeling.model import MajorityLabelVoter, LabelModel\n",
    "from snorkel.analysis import get_label_buckets\n",
    "\n",
    "majority_model = MajorityLabelVoter()\n",
    "label_model = LabelModel(cardinality=2, verbose=True)\n",
    "\n",
    "import helper as hlp\n",
    "import importlib\n",
    "importlib.reload(hlp)\n",
    "\n",
    "# global variables\n",
    "ABSTAIN = -1; CONTROL = 0; CASE = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load raw & aggregated (grouped) data\n",
    "df, dfg = hlp.load_data()\n",
    "\n",
    "# add numeric data\n",
    "dfg = hlp.add_numeric_data(dfg)\n",
    "\n",
    "# train/deve/valid/test split\n",
    "df_train, df_dev, df_valid, df_test = hlp.make_splits(dfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import from chart review\n",
    "df_dev_labeled = pd.read_csv('./dev_set_labeled.csv')\n",
    "df_dev = df_dev.merge(df_dev_labeled[['visit_occurrence_id', 'label']], on='visit_occurrence_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store Y values for ease of evaluation\n",
    "Y_dev = df_dev['label'].values\n",
    "Y_dev = np.where(Y_dev=='case', 1, 0) \n",
    "\n",
    "#Y_valid = df_valid['label'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Round 1 - Attempt to Expand Dev & Valid Sets\n",
    "\n",
    "Even though I attempted to oversample from the cases during the creation of the development & validation sets, upon manual review, I only had 2 actual positive cases in the development set. Therefore, let's start with what I learned from review those 50 encounters and see about applying learning functions (LFs) to the training set & using the most likely encounters to enrich the development & validation sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionary to keep track of rule names for easier reference later\n",
    "lfd = dict()\n",
    "\n",
    "@labeling_function()\n",
    "def LF_naloxone_admin(x):\n",
    "    if x['naloxone_admin_prob'] >= 0.75:\n",
    "        return CASE\n",
    "    elif x['naloxone_admin_prob'] < 0.75:\n",
    "        return CONTROL\n",
    "    else:\n",
    "        # if missing\n",
    "        return ABSTAIN\n",
    "lfd['LF_naloxone_admin'] = 0\n",
    "    \n",
    "@labeling_function()\n",
    "def LF_respiratory_failure_any(x):\n",
    "    if '1' in x['respiratory_failure_any'].lower(): \n",
    "        return CASE\n",
    "    return CONTROL\n",
    "lfd['LF_respiratory_failure_any'] = max(lfd.values()) + 1\n",
    "\n",
    "@labeling_function()\n",
    "def LF_eligible_vent(x):\n",
    "    if 'yes' in x['eligible_vent'].lower(): \n",
    "        return CASE\n",
    "    return ABSTAIN\n",
    "lfd['LF_eligible_vent'] = max(lfd.values()) + 1\n",
    "\n",
    "@labeling_function()\n",
    "def LF_counts_naloxone(x):\n",
    "    if x['counts_naloxone'] > 0: \n",
    "        return CASE\n",
    "    return ABSTAIN\n",
    "lfd['LF_counts_naloxone'] = max(lfd.values()) + 1\n",
    "\n",
    "@labeling_function()\n",
    "def LF_counts_resp_care_notes(x):\n",
    "    if x['counts_resp_care_notes'] == 0:\n",
    "        return CONTROL\n",
    "    return ABSTAIN\n",
    "lfd['LF_counts_resp_care_notes'] = max(lfd.values()) + 1\n",
    "\n",
    "@labeling_function()\n",
    "def LF_counts_rapid_response(x):\n",
    "    if x['counts_rapid_response'] > 0:\n",
    "        return CASE\n",
    "    return CONTROL\n",
    "lfd['LF_counts_rapid_response'] = max(lfd.values()) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all relevant LFs\n",
    "lfs = [LF_naloxone_admin,\n",
    "      LF_respiratory_failure_any,\n",
    "      LF_eligible_vent,\n",
    "      LF_counts_naloxone,\n",
    "      LF_counts_resp_care_notes,\n",
    "      LF_counts_rapid_response]\n",
    "\n",
    "# apply LFs\n",
    "applier = PandasLFApplier(lfs=lfs)\n",
    "L_train = applier.apply(df=df_train)\n",
    "L_dev = applier.apply(df=df_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LFAnalysis(L=L_train, lfs=lfs).lf_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LFAnalysis(L=L_dev, lfs=lfs).lf_summary(Y=Y_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.iloc[L_train[:, lfd['LF_counts_rapid_response']] == CASE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dev.iloc[L_dev[:, lfd['LF_naloxone_admin']] == CASE]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conflicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buckets = get_label_buckets(L_dev[:, lfd['LF_respiratory_failure_any']], L_dev[:, lfd['LF_counts_rapid_response']])\n",
    "df_dev.iloc[buckets[(CASE, CONTROL)]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we're only getting rid of `eligible_vent` because it was wrong in every instance. We might add it back in later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lfs.remove(LF_eligible_vent)\n",
    "lfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "applier = PandasLFApplier(lfs=lfs)\n",
    "L_train = applier.apply(df=df_train)\n",
    "L_dev = applier.apply(df=df_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure the code can run\n",
    "label_model.fit(L_train=L_train, Y_dev = Y_dev, \n",
    "                n_epochs = 4000, lr = 0.004, #l2 = 0.01,\n",
    "                optimizer = 'adamax', lr_scheduler = 'step', #prec_init = 0.7,\n",
    "                log_freq = 100, seed = 987)\n",
    "analysis = LFAnalysis(L=L_train, lfs=lfs).lf_summary(est_weights=label_model.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create empty dataframe to hold learned weights for each rule\n",
    "df_cols = ['n_epochs', 'lr', 'lr_scheduler']\n",
    "df_cols.extend(analysis.index)\n",
    "\n",
    "# specify potential hyperparameters\n",
    "n_epochs = [2000, 4000]\n",
    "lr = [0.001, 0.005, 0.01]\n",
    "lr_scheduler = ['step', 'exponential', 'linear']\n",
    "\n",
    "# tune\n",
    "df_tune, df_tune_long = hlp.label_model_tuning(lfs, df_cols, \n",
    "                                               L_train, L_dev, Y_dev, \n",
    "                                               n_epochs, lr, lr_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# review best accuracies among development set\n",
    "df_tune[df_tune['accuracy'] == np.max(df_tune['accuracy'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for scheduler in lr_scheduler:\n",
    "    g = sns.FacetGrid(df_tune_long[df_tune_long['lr_scheduler']==scheduler], \n",
    "                      col='lr', hue='learning_function', col_wrap=3, height=4)\n",
    "    g = (g.map(plt.scatter, 'n_epochs', 'learned_weight')\n",
    "            .add_legend()\n",
    "            .fig.suptitle('Learned Weights Using ' + str(scheduler) + ' Scheduler', \n",
    "                          y=1.05, fontsize=16))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation: `counts_resp_care_notes` and `counts_rapid_response` were fairly accurate & had good coverage. `naloxone_admin` was also pretty accurate & makes sense as being important. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_model.fit(L_train=L_train, Y_dev = Y_dev, n_epochs = 2000, lr = 0.01, optimizer = 'adamax', \n",
    "                lr_scheduler = 'step', log_freq = 100, seed = 987)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LFAnalysis(L=L_train, lfs=lfs) \\\n",
    "    .lf_summary(est_weights = label_model.get_weights()) \\\n",
    "    .sort_values(by='Learned Weight', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "majority_acc = majority_model.score(L=L_dev, Y=Y_dev)[\"accuracy\"]\n",
    "print(f\"{'Majority Vote Accuracy:':<25} {majority_acc * 100:.1f}%\")\n",
    "\n",
    "label_model_acc = label_model.score(L=L_dev, Y=Y_dev)[\"accuracy\"]\n",
    "print(f\"{'Label Model Accuracy:':<25} {label_model_acc * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign probabilities from either majority vote or label model\n",
    "#gen_probs_train = majority_model.predict_proba(L=L_train)\n",
    "#gen_probs_dev = majority_model.predict_proba(L=L_dev)\n",
    "\n",
    "gen_probs_train = label_model.predict_proba(L=L_train)\n",
    "gen_probs_dev = label_model.predict_proba(L=L_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hlp.plot_probabilities_histogram(gen_probs_train[:, CASE])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hlp.plot_probabilities_histogram(gen_probs_dev[:, CASE])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attach LabelModel predictions to dataframes\n",
    "train_with_probs = df_train.copy()\n",
    "train_with_probs['label_model_prob'] = gen_probs_train[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract top 20 highest probabilities from train set\n",
    "top_probs = train_with_probs.nlargest(n=20, columns='label_model_prob')\n",
    "\n",
    "# send half to the dev set & half to the valid set\n",
    "visits_for_dev = top_probs['visit_occurrence_id'].sample(frac=0.5, random_state=123)\n",
    "visits_for_valid = top_probs[~np.isin(top_probs['visit_occurrence_id'], visits_for_dev)]['visit_occurrence_id']\n",
    "\n",
    "# concatenate the respective training set rows to dev & valid sets\n",
    "df_dev2 = pd.concat([df_dev, df_train[df_train['visit_occurrence_id'].isin(visits_for_dev)]], sort=True)\n",
    "df_valid2 = pd.concat([df_valid, df_train[np.isin(df_train['visit_occurrence_id'], visits_for_valid)]], sort=True)\n",
    "\n",
    "# remove the rows from the training set\n",
    "df_train2 = df_train.drop(top_probs.index)\n",
    "\n",
    "assert df_dev2.shape[0] == df_dev.shape[0] + 0.5*top_probs.shape[0]\n",
    "assert df_valid2.shape[0] == df_valid.shape[0] + 0.5*top_probs.shape[0]\n",
    "assert df_train2.shape[0] == df_train.shape[0] - top_probs.shape[0]\n",
    "assert not np.isin(df_train2['visit_occurrence_id'], df_dev2['visit_occurrence_id']).any()\n",
    "assert not np.isin(df_train2['visit_occurrence_id'], df_valid2['visit_occurrence_id']).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for manual chart review, attach new cases to the dev (and/or valid set)\n",
    "\n",
    "#df_dev2_labeled = pd.concat([df_dev_labeled, \n",
    "#                             df_train[df_train['visit_occurrence_id'].isin(visits_for_dev)]], \n",
    "#                            sort=True)\n",
    "\n",
    "#df_valid2_labeled = pd.concat([df_valid_labeled, \n",
    "#                               df_train[df_train['visit_occurrence_id'].isin(visits_for_valid)]], \n",
    "#                            sort=True)\n",
    "\n",
    "# export\n",
    "#df_train2.to_csv('./train_set2.csv', index=False)\n",
    "#df_dev2_labeled.to_csv('./dev_set2_labeled.csv', index=False)\n",
    "#df_valid2_labeled.to_csv('./valid_set2_labeled.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
