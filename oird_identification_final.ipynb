{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SnorkelMED - Identifying Opioid-Induced Respiratory Depression  \n",
    "\n",
    "The purpose of this analysis is to probabilistically identify which patient visits included an opioid-induced respiratory depression (OIRD) event. In this notebook, we use our LFs and the validation set to develop the deterministic model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from snorkel.labeling import labeling_function, PandasLFApplier, LFAnalysis, filter_unlabeled_dataframe\n",
    "from snorkel.labeling.model import MajorityLabelVoter, LabelModel\n",
    "from snorkel.analysis import get_label_buckets, metric_score\n",
    "from snorkel.utils import probs_to_preds\n",
    "\n",
    "majority_model = MajorityLabelVoter()\n",
    "label_model = LabelModel(cardinality=2, verbose=True)\n",
    "\n",
    "import helper as hlp\n",
    "import importlib\n",
    "importlib.reload(hlp)\n",
    "\n",
    "# global variables\n",
    "ABSTAIN = -1; CONTROL = 0; CASE = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load updated training/dev/valid data after labeling from previous round\n",
    "df_train, df_dev, df_valid, df_test = hlp.load_data(round=5)\n",
    "\n",
    "# re-attach numeric data to reflect any updated rules\n",
    "df_train, df_dev, df_valid, df_test = hlp.reattach_numeric_data(df_train, df_dev, df_valid, df_test)\n",
    "\n",
    "# keep confounding diagnoses visits available\n",
    "confounding_diagnosis_present = pd.read_csv('../sd_structured/icd/visits_with_confounding_icd_codes.csv')\n",
    "\n",
    "# made changes in code in to loading confounding diagnoses, so eliminating the redundant columns in corrected\n",
    "#    validation set\n",
    "df_valid.drop(['condition_start_date', 'cva', 'sepsis'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store Y values for ease of evaluation\n",
    "Y_dev = df_dev['label'].values\n",
    "Y_dev = np.where(Y_dev=='case', 1, 0) \n",
    "\n",
    "Y_valid = df_valid['label'].values\n",
    "Y_valid = np.where(Y_valid=='case', 1, 0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Generative Labels to Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copied directly from last round of learning function applications\n",
    "# removes LFs that were ultimately dropped\n",
    "lfd = dict()\n",
    "\n",
    "@labeling_function()\n",
    "def LF_naloxone_admin(x):\n",
    "    if x['naloxone_admin_prob'] >= 0.8:\n",
    "        if x['counts_naloxone_NOT_effective'] > x['counts_naloxone_effective']:\n",
    "            return CONTROL\n",
    "        elif x['counts_naloxone_effective'] > 0:\n",
    "            return CASE\n",
    "        elif x['counts_naloxone_NOT_effective'] > 0:\n",
    "            return CONTROL\n",
    "        else:\n",
    "            return CASE\n",
    "    elif x['naloxone_admin_prob'] < 0.8:\n",
    "        return CONTROL\n",
    "    else:\n",
    "        return CONTROL\n",
    "lfd['LF_naloxone_admin'] = 0\n",
    "\n",
    "@labeling_function()\n",
    "def LF_counts_naloxone(x):\n",
    "    if x['counts_naloxone'] > 0: \n",
    "        if x['counts_naloxone_NOT_effective'] > x['counts_naloxone_effective']:\n",
    "            return CONTROL\n",
    "        elif x['counts_naloxone_effective'] > 0:\n",
    "            return CASE\n",
    "        elif x['counts_naloxone_NOT_effective'] > 0:\n",
    "            return CONTROL\n",
    "        else:\n",
    "            return CASE\n",
    "    return ABSTAIN\n",
    "lfd['LF_counts_naloxone'] = max(lfd.values()) + 1\n",
    "\n",
    "@labeling_function()\n",
    "def LF_respiratory_failure_any(x):\n",
    "    if '1' in x['respiratory_failure_any'].lower(): \n",
    "        if 'yes' in x['eligible_vent'].lower():\n",
    "            return CONTROL\n",
    "    return ABSTAIN\n",
    "lfd['LF_respiratory_failure_any'] = max(lfd.values()) + 1\n",
    "\n",
    "@labeling_function()\n",
    "def LF_counts_resp_care_notes(x):\n",
    "    if x['counts_resp_care_notes'] == 0:\n",
    "        return CONTROL\n",
    "    return ABSTAIN\n",
    "lfd['LF_counts_resp_care_notes'] = max(lfd.values()) + 1\n",
    "\n",
    "@labeling_function()\n",
    "def LF_counts_no_acute_events(x):\n",
    "    if x['counts_no_acute_events'] > 0:\n",
    "        if x['counts_naloxone'] + x['counts_rapid_response'] + x['counts_altered_mental_status'] + \\\n",
    "            x['counts_narcotic_overdose'] + x['counts_hypoxia'] + x['counts_held_opioids'] > 0:\n",
    "            return ABSTAIN\n",
    "        return CONTROL\n",
    "    return ABSTAIN\n",
    "lfd['LF_counts_no_acute_events'] = max(lfd.values()) + 1\n",
    "\n",
    "@labeling_function()\n",
    "def LF_counts_altered_mental_status(x):\n",
    "    if x['counts_altered_mental_status'] > 0:\n",
    "        if x['visit_occurrence_id'] in confounding_diagnosis_present['visit_occurrence_id'].unique(): \n",
    "            return ABSTAIN\n",
    "        return CASE\n",
    "    return ABSTAIN\n",
    "lfd['LF_counts_altered_mental_status'] = max(lfd.values()) + 1\n",
    "\n",
    "@labeling_function()\n",
    "def LF_counts_narcotic_overdose(x):\n",
    "    if x['counts_narcotic_overdose'] > 0:\n",
    "        return CASE\n",
    "    return ABSTAIN\n",
    "lfd['LF_counts_narcotic_overdose'] = max(lfd.values()) + 1\n",
    "\n",
    "@labeling_function()\n",
    "def LF_counts_hypoxia(x):\n",
    "    if x['counts_hypoxia'] > 0:\n",
    "        return CASE\n",
    "    return ABSTAIN\n",
    "lfd['LF_counts_hypoxia'] = max(lfd.values()) + 1\n",
    "\n",
    "@labeling_function()\n",
    "def LF_counts_decrease_opioids(x):\n",
    "    if x['counts_decrease_opioids'] > 0:\n",
    "        return CASE\n",
    "    return ABSTAIN\n",
    "lfd['LF_counts_decrease_opioids'] = max(lfd.values()) + 1\n",
    "\n",
    "@labeling_function()\n",
    "def LF_extended_vent_time(x):\n",
    "    if 'yes;;yes;;yes;;yes' in x['eligible_vent'].lower(): \n",
    "        return CONTROL\n",
    "    return ABSTAIN\n",
    "lfd['LF_extended_vent_time'] = max(lfd.values()) + 1\n",
    "\n",
    "@labeling_function()\n",
    "def LF_counts_no_pain_meds(x):\n",
    "    if x['counts_no_pain_meds'] > 0:\n",
    "        return CONTROL\n",
    "    return ABSTAIN\n",
    "lfd['LF_counts_no_pain_meds'] = max(lfd.values()) + 1\n",
    "\n",
    "@labeling_function()\n",
    "def LF_no_support(x):\n",
    "    if x['counts_naloxone'] == 0 and \\\n",
    "        np.isnan(x['naloxone_admin_prob']) and \\\n",
    "        x['counts_altered_mental_status'] == 0 and \\\n",
    "        x['counts_narcotic_overdose'] == 0 and \\\n",
    "        x['counts_hypoxia'] == 0 and \\\n",
    "        x['counts_decrease_opioids'] == 0 and \\\n",
    "        x['counts_rapid_response'] == 0 and \\\n",
    "        x['counts_held_opioids'] == 0 and \\\n",
    "        x['counts_pinpoint_pupils'] == 0 and \\\n",
    "        '1' not in x['respiratory_failure_any'].lower():\n",
    "        return CONTROL\n",
    "    return ABSTAIN\n",
    "lfd['LF_no_support'] = max(lfd.values()) + 1\n",
    "\n",
    "@labeling_function()\n",
    "def LF_counts_held_opioids(x):\n",
    "    if x['counts_held_opioids'] > 0:\n",
    "        return CASE\n",
    "    return ABSTAIN\n",
    "lfd['LF_counts_held_opioids'] = max(lfd.values()) + 1\n",
    "\n",
    "@labeling_function()\n",
    "def LF_counts_pinpoint_pupils(x):\n",
    "    if x['counts_pinpoint_pupils'] > 0:\n",
    "        return CASE\n",
    "    return ABSTAIN\n",
    "lfd['LF_counts_pinpoint_pupils'] = max(lfd.values()) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lfs = [LF_naloxone_admin, LF_counts_naloxone, LF_respiratory_failure_any, LF_counts_resp_care_notes,\n",
    "       LF_counts_no_acute_events, LF_counts_altered_mental_status, LF_counts_narcotic_overdose,\n",
    "       LF_counts_hypoxia, LF_counts_decrease_opioids, LF_extended_vent_time, LF_counts_no_pain_meds,\n",
    "       LF_no_support, LF_counts_held_opioids, LF_counts_pinpoint_pupils]\n",
    "\n",
    "applier = PandasLFApplier(lfs=lfs)\n",
    "L_train = applier.apply(df=df_train)\n",
    "L_dev = applier.apply(df=df_dev)\n",
    "L_valid = applier.apply(df=df_valid)\n",
    "L_test = applier.apply(df=df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# combine train & dev sets into new training set\n",
    "df_train_dev = df_train.append(df_dev, sort=False)\n",
    "\n",
    "L_train_dev = applier.apply(df=df_train_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best hyper-parameters from most current LF notebook\n",
    "n_epochs = 2000\n",
    "lr = 0.01 \n",
    "lr_scheduler = 'step'\n",
    "seed = 987 \n",
    "class_balance = [0.985, 0.015] # major class imabalance (5-15 events per 1,000 surgical cases, so 0.005-0.015)\n",
    "\n",
    "# replace Y_dev with Y_valid in this notebook\n",
    "label_model.fit(L_train = L_train_dev, Y_dev = Y_valid, n_epochs = n_epochs, lr = lr, optimizer = 'adamax', \n",
    "                lr_scheduler = lr_scheduler, log_freq = 100, seed = seed, \n",
    "                class_balance = class_balance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FYI, these numbers will look slightly different because there are 20 fewer patients in the training set\n",
    "#    after removing the top-scoring patients & placing in dev & valid sets, and then the 90-patient dev\n",
    "#    sets get appended back to the train set\n",
    "#    But the numbers look pretty similar to before\n",
    "LFAnalysis(L=L_train_dev, lfs=lfs) \\\n",
    "    .lf_summary(est_weights = label_model.get_weights()) \\\n",
    "    .sort_values(by='Learned Weight', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empirical accuracy within Validation set\n",
    "LFAnalysis(L=L_valid, lfs=lfs).lf_summary(Y=Y_valid).sort_values('Emp. Acc.', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance_lf(L, Y):\n",
    "    \"\"\" Helper function for printing evaluation of performance. \"\"\"\n",
    "    for model in ['maj', 'lab']:\n",
    "        if model == 'maj':\n",
    "            print(\"Majority Vote...\")\n",
    "            acc = majority_model.score(L=L, Y=Y)['accuracy']\n",
    "            probs = majority_model.predict_proba(L)\n",
    "        elif model == 'lab':\n",
    "            print(\"Label Model...\")\n",
    "            acc = label_model.score(L=L, Y=Y)['accuracy']\n",
    "            probs = label_model.predict_proba(L)\n",
    "            \n",
    "        preds = probs_to_preds(probs)\n",
    "        print(f\"{'Accuracy:':<10} {acc * 100:.1f}%\")\n",
    "        print(f\"{'F1 score:':<10} {metric_score(Y, preds, probs=probs, metric='f1') * 1:.3f}\")\n",
    "        print(f\"{'AUC:':<10} {metric_score(Y, preds, probs=probs, metric='roc_auc') * 1:.3f}\", '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_lf(L_dev, Y_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The label model outperforms the majority vote in all metrics for the dev set, but this is also over-fit since we included in the dev set in the train set now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_lf(L_valid, Y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The majority vote does a bit better on the validation set, with respect to accuracy & F1 score. The AUC is higher in the Label Model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign probabilities from either majority vote or label model\n",
    "gen_probs_train = majority_model.predict_proba(L=L_train_dev)\n",
    "gen_probs_dev = majority_model.predict_proba(L=L_dev)\n",
    "gen_probs_valid = majority_model.predict_proba(L=L_valid)\n",
    "\n",
    "hlp.plot_probabilities_histogram(gen_probs_valid[:, CASE])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_probs_train = label_model.predict_proba(L=L_train_dev)\n",
    "gen_probs_dev = label_model.predict_proba(L=L_dev)\n",
    "gen_probs_valid = label_model.predict_proba(L=L_valid)\n",
    "\n",
    "hlp.plot_probabilities_histogram(gen_probs_valid[:, CASE])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how do these compare? \n",
    "maj = majority_model.predict_proba(L=L_valid)\n",
    "lab = label_model.predict_proba(L=L_valid)\n",
    "\n",
    "plt.scatter(lab, maj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Relevant Data for Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_data = (df_train_dev, df_valid, df_test, Y_dev, Y_valid, \n",
    "               L_train, L_dev, L_train_dev, L_valid, \n",
    "               label_model, majority_model, L_test)\n",
    "\n",
    "with open('data_for_analysis.pkl', 'wb') as f:\n",
    "    pickle.dump(export_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
