{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold, GridSearchCV, cross_validate\n",
    "from sklearn import model_selection, metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "# snorkel\n",
    "from snorkel.labeling import labeling_function, PandasLFApplier, LFAnalysis, filter_unlabeled_dataframe\n",
    "from snorkel.labeling.model import MajorityLabelVoter, LabelModel\n",
    "from snorkel.analysis import get_label_buckets, metric_score\n",
    "from snorkel.utils import probs_to_preds\n",
    "\n",
    "majority_model = MajorityLabelVoter()\n",
    "label_model = LabelModel(cardinality=2, verbose=True)\n",
    "\n",
    "import helper as hlp\n",
    "import importlib\n",
    "importlib.reload(hlp)\n",
    "\n",
    "# global variables\n",
    "ABSTAIN = -1; CONTROL = 0; CASE = 1\n",
    "SEED = 987"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data, which was saved as a tuple...\n",
    "#export_data = (df_train_dev, df_valid, df_test, Y_dev, Y_valid,\n",
    "#               L_train, L_dev, L_train_dev, L_valid,  \n",
    "#               label_model, majority_model, L_test)\n",
    "\n",
    "with open('./data_for_analysis.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "    \n",
    "df_train_dev = data[0]; df_valid = data[1]; df_test = data[2]\n",
    "Y_dev = data[3]; Y_valid = data[4]\n",
    "L_train = data[5]; L_dev = data[6]; L_train_dev = data[7]; L_valid = data[8]\n",
    "label_model = data[9]; majority_model = data[10]\n",
    "L_test = data[11]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Model Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outcome  \n",
    "Although we're primarily depending on the Generative model for labels, we can still leverage our manually adjudicated information for more robust information - something is better than nothing, right? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find observed values from label model probabilities that are closest to 0 or 1 and make \n",
    "#    manually-adjudicated labels slightly closer to 0 or 1, respectively\n",
    "label_model_probs = label_model.predict_proba(L_train_dev)[:, CASE]\n",
    "lower_limit = 0.95 * np.min(label_model_probs)\n",
    "upper_limit = 0.95 * (1-np.max(label_model_probs)) + np.max(label_model_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store on dataframe, using manual adjudication if available\n",
    "df_train_dev['outcome_generative_model'] = label_model_probs\n",
    "df_train_dev['outcome'] = np.where(pd.isnull(df_train_dev['label']), # if label is missing...\n",
    "                                           # use generative model\n",
    "                                           df_train_dev['outcome_generative_model'], \n",
    "                                           # otherwise, use manually-adjudicated label \n",
    "                                           # but with offset for regression-based models\n",
    "                                           np.where(df_train_dev['label']=='case', upper_limit, lower_limit))\n",
    "\n",
    "# create y variables\n",
    "y_train_probs = np.array(df_train_dev['outcome'])\n",
    "y_train_preds = np.where(df_train_dev['outcome'] >= 0.5, 1, 0)\n",
    "\n",
    "y_valid_probs = label_model.predict_proba(L_valid)#[:, CASE] # only used as FYI\n",
    "y_valid_preds = probs_to_preds(y_valid_probs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution of training set probabilities\n",
    "plt.hist(y_train_probs, bins=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log-transformed distribution\n",
    "assert np.min(y_train_probs > 0)\n",
    "plt.hist(np.log(y_train_probs), bins=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# event rate\n",
    "np.mean(y_train_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(y_train_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FYI: Generative Model Performance on Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval = pd.DataFrame({'predicted': np.round(y_valid_probs[:, CASE], 2), \n",
    "                     'actual': np.where(Y_valid==0, 'Control', 'Case')})\n",
    "eval = eval.sort_values(by=['predicted', 'actual'])\n",
    "eval = eval.assign(counts =eval.groupby(['predicted']).cumcount())\n",
    "\n",
    "fig = sns.scatterplot(data=eval, x=\"predicted\", y=\"counts\", \n",
    "                      hue=eval[\"actual\"].tolist(), palette=\"colorblind\", s=100)\n",
    "plt.title('Generative Model Performance in Validation Set (F1=0.73, AUC=0.96)')\n",
    "plt.ylabel('Visit Counts (n)')\n",
    "plt.ylim([-1, 40])\n",
    "plt.xlabel('Probability(OIRD)')\n",
    "plt.legend(loc='upper center')\n",
    "sns.set(rc={'figure.figsize': (15, 5), 'figure.dpi': 300})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(Y_valid, y_valid_preds, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictors  \n",
    "\n",
    "For the deterministic model, we're keeping a generalizable set of features. We could depend on the previously-developed learning functions, but one draw-back is the amount of feature engineering that's put into that. Alternatively, we can start with the raw features, e.g., age, regular expression counts, etc. It might also be unwise to use the `nalxone_admin_prob` value due to it being created with a previous Snorkel model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull in original naloxone administration info & only count \"received\" if \"epic ip admin\" or \"hed\" present\n",
    "naloxone = pd.read_csv('../sd_structured/meds/naloxone/naloxone_exposure_pre.csv', sep='\\t')\n",
    "naloxone.columns = naloxone.columns.str.lower()\n",
    "\n",
    "# collapse all visit day onto a single row\n",
    "SEP = ';;'\n",
    "join_as_strings = lambda x: SEP.join(map(str, x))\n",
    "\n",
    "naloxone = naloxone.groupby(['visit_occurrence_id', 'grid', 'label']) \\\n",
    "    ['x_frequency', 'drug_source_value', 'x_doc_type', 'x_doc_stype'] \\\n",
    "    .agg(join_as_strings) \\\n",
    "    .reset_index()\n",
    "\n",
    "# create binary indicator of whether naloxone received based on simple rule\n",
    "naloxone['binary_naloxone_admin'] = np.where((naloxone['x_doc_type'].str.contains('HED')) | \n",
    "                                            (naloxone['x_doc_type'].str.contains('EPIC IP ADMIN')),\n",
    "                                            1, 0)\n",
    "\n",
    "# attach to train/dev and validation sets\n",
    "df_train_dev = df_train_dev.merge(naloxone[['visit_occurrence_id', 'binary_naloxone_admin']], \n",
    "                                  how='left', on=['visit_occurrence_id'])\n",
    "df_valid = df_valid.merge(naloxone[['visit_occurrence_id', 'binary_naloxone_admin']], \n",
    "                          how='left', on=['visit_occurrence_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create numeric columns from string-based columns\n",
    "df_train_dev['binary_respiratory_failure_any'] = \\\n",
    "    np.where(df_train_dev['respiratory_failure_any'].str.contains('1'), 1, 0)\n",
    "df_valid['binary_respiratory_failure_any'] = \\\n",
    "    np.where(df_valid['respiratory_failure_any'].str.contains('1'), 1, 0)\n",
    "\n",
    "df_train_dev['binary_eligible_vent'] = \\\n",
    "    np.where(df_train_dev['eligible_vent'].str.contains('Yes'), 1, 0)\n",
    "df_valid['binary_eligible_vent'] = \\\n",
    "    np.where(df_valid['eligible_vent'].str.contains('Yes'), 1, 0)\n",
    "\n",
    "# coerce only categorical column into binary\n",
    "df_train_dev['binary_gender_female'] = np.where(df_train_dev['gender']=='FEMALE', 1, 0)\n",
    "df_valid['binary_gender_female'] = np.where(df_valid['gender']=='FEMALE', 1, 0)\n",
    "\n",
    "# replace missing values from naloxone join with \"0\"\n",
    "df_train_dev = df_train_dev.fillna(value={'binary_naloxone_admin': 0})\n",
    "df_valid = df_valid.fillna(value={'binary_naloxone_admin': 0})\n",
    "\n",
    "# replace NaN values with 0 for ICD conditions\n",
    "df_train_dev['binary_cond_resp_failure'] = np.where(df_train_dev['cond_resp_failure']==1, 1, 0)\n",
    "df_valid['binary_cond_resp_failure'] = np.where(df_valid['cond_resp_failure']==1, 1, 0)\n",
    "\n",
    "df_train_dev['binary_cond_sepsis'] = np.where(df_train_dev['cond_sepsis']==1, 1, 0)\n",
    "df_valid['binary_cond_sepsis'] = np.where(df_valid['cond_sepsis']==1, 1, 0)\n",
    "\n",
    "df_train_dev['binary_cond_cva'] = np.where(df_train_dev['cond_cva']==1, 1, 0)\n",
    "df_valid['binary_cond_cva'] = np.where(df_valid['cond_cva']==1, 1, 0)\n",
    "\n",
    "df_train_dev['binary_cond_resp_disease'] = np.where(df_train_dev['cond_resp_disease']==1, 1, 0)\n",
    "df_valid['binary_cond_resp_disease'] = np.where(df_valid['cond_resp_disease']==1, 1, 0)\n",
    "\n",
    "df_train_dev['binary_cond_cv_disease'] = np.where(df_train_dev['cond_cv_disease']==1, 1, 0)\n",
    "df_valid['binary_cond_cv_disease'] = np.where(df_valid['cond_cv_disease']==1, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Data Matrices  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify columns for model building\n",
    "cols_binary = df_train_dev.columns[df_train_dev.columns.str.contains('binary_')]\n",
    "cols_counts = df_train_dev.columns[df_train_dev.columns.str.contains('counts_')]\n",
    "\n",
    "cols = ['age_on_admission'] #'naloxone_admin_prob'\n",
    "cols.extend(cols_binary)\n",
    "cols.extend(cols_counts)\n",
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset columns\n",
    "X_train = df_train_dev[cols]\n",
    "X_valid = df_valid[cols]\n",
    "\n",
    "# also, some of the \"counts\" variables didn't have any results because those patients didn't have charts\n",
    "#    consider imputing \"0\" here, too\n",
    "X_train.fillna(0, inplace=True)\n",
    "X_valid.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale data \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_valid = sc.transform(X_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Deterministic Model(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifiers: Off-the-Shelf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin with off-the-shelf models that don't require hyper-parameter tuning at this time. This will give us a start in determining which models might be worth hyper-parameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare models\n",
    "models = []\n",
    "models.append(('LR', LogisticRegression()))\n",
    "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('CART', DecisionTreeClassifier()))\n",
    "models.append(('RF', RandomForestClassifier()))\n",
    "models.append(('NB', GaussianNB()))\n",
    "models.append(('NN', MLPClassifier()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate each model in turn\n",
    "results = dict()\n",
    "names = []\n",
    "scoring = ('f1', 'roc_auc', 'neg_mean_squared_error')\n",
    "y = y_train_preds # binary classification\n",
    "\n",
    "for name, model in models:\n",
    "    kfold = model_selection.KFold(n_splits=5, random_state=SEED, shuffle=True)\n",
    "    # apply multiple scoring metrics\n",
    "    cv_results = model_selection.cross_validate(model, X_train, y, cv=kfold, scoring=scoring, n_jobs=-1)\n",
    "    # store results\n",
    "    names.append(name)\n",
    "    results[name] = cv_results\n",
    "    for score in scoring:\n",
    "        print('{:<5s}{:<25s}{:^15.3f}{:>5.3f}'.format(name, score, cv_results['test_' + score].mean(), \n",
    "                                                      cv_results['test_' + score].std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# algorithm comparison\n",
    "for score in scoring:\n",
    "    results_partial = []\n",
    "    for name in names:\n",
    "        results_partial.append(results[name]['test_' + score])\n",
    "    fig = plt.figure()\n",
    "    fig.suptitle('Algorithm Comparison based on ' + score)\n",
    "    ax = fig.add_subplot(111)\n",
    "    plt.boxplot(results_partial)\n",
    "    plt.ylabel(score)\n",
    "    ax.set_xticklabels(names)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using off-the-shelf models with no hyper-parameter tuning, it appears the LDA, Random Forest, & Neural Network are the best models across F1 & AUC metrics. I've never seen LDA out-perform RF, so I'll tune RF & NN classifiers further. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regressors: Off-the-Shelf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "models.append(('LR', LinearRegression()))\n",
    "models.append(('RF', RandomForestRegressor()))\n",
    "models.append(('NN', MLPRegressor()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "names = []\n",
    "scoring = 'neg_mean_squared_error' \n",
    "y = np.log(y_train_probs)\n",
    "\n",
    "for name, model in models:\n",
    "    kfold = model_selection.KFold(n_splits=5, random_state=SEED, shuffle=True)\n",
    "    cv_results = model_selection.cross_val_score(model, X_train, y, \n",
    "                                                 cv=kfold, scoring=scoring)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "fig.suptitle('Algorithm Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(results)\n",
    "plt.ylabel(scoring)\n",
    "ax.set_xticklabels(names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the classifiers, it looks like RF & NN are the best performers, and we could try more hyper-parameter tuning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sk_rfr = RandomForestRegressor()\n",
    "sk_rfr.fit(X=X_train, y=np.log(y_train_probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba = np.exp(sk_rfr.predict(X_valid)) # convert back to probability\n",
    "\n",
    "eval = pd.DataFrame({'predicted': np.round(y_pred_proba, 2), 'actual': Y_valid})\n",
    "eval = eval.sort_values(by=['predicted', 'actual'])\n",
    "eval = eval.assign(counts =eval.groupby(['predicted']).cumcount())\n",
    "\n",
    "fig = sns.scatterplot(data=eval, x=\"predicted\", y=\"counts\", \n",
    "                      hue=eval[\"actual\"].tolist(), palette=\"colorblind\", s=100)\n",
    "plt.ylabel('Counts')\n",
    "plt.xlabel('Predicted Value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assess classifier-like metrics\n",
    "y_pred = np.where(y_pred_proba >= 0.5, 1, 0)\n",
    "\n",
    "print(metrics.classification_report(Y_valid, y_pred, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attempt sample weights with by downweighting probabilities closer to 0.5\n",
    "weights = np.abs(df_train_dev['outcome_generative_model']-0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sk_rfr = RandomForestRegressor()\n",
    "sk_rfr.fit(X=X_train, y=np.log(y_train_probs), sample_weight=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba = np.exp(sk_rfr.predict(X_valid)) # convert back to probability\n",
    "\n",
    "eval = pd.DataFrame({'predicted': np.round(y_pred_proba, 2), 'actual': Y_valid})\n",
    "eval = eval.sort_values(by=['predicted', 'actual'])\n",
    "eval = eval.assign(counts =eval.groupby(['predicted']).cumcount())\n",
    "\n",
    "fig = sns.scatterplot(data=eval, x=\"predicted\", y=\"counts\", \n",
    "                      hue=eval[\"actual\"].tolist(), palette=\"colorblind\", s=100)\n",
    "plt.ylabel('Counts')\n",
    "plt.xlabel('Predicted Value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.where(y_pred_proba >= 0.5, 1, 0)\n",
    "print(metrics.classification_report(Y_valid, y_pred, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nested Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('./cv_results.pkl'):\n",
    "    cv_results = pickle.load(open('./cv_results.pkl', 'rb'))\n",
    "else:\n",
    "    cv_results = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global values\n",
    "K_i = 3         # inner folds\n",
    "K_o = 10        # outer folds\n",
    "verbosity = 0   # do not print when exploring full set of values\n",
    "\n",
    "# test values for debugging\n",
    "#K_i = 3\n",
    "#K_o = 3\n",
    "#verbosity = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(hlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier - Unweighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier()\n",
    "params = [{'n_estimators': [250],\n",
    "           'class_weight': [None, 'balanced', 'balanced_subsample', {0: 0.99, 1: 0.01}, {0: 0.01, 1: 0.99}],\n",
    "           'max_depth': [None, 5, 10, 50],\n",
    "           'max_features': [None, 'sqrt', 'log2']}]\n",
    "\n",
    "rfc = hlp.nested_cv(X_train, y_train_preds, model, params, \n",
    "                    k_outer=K_o, k_inner=K_i, verbosity=verbosity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save values in directory\n",
    "name = 'RandomForestClassifier-Unweighted'\n",
    "cv_results[name] = {'model': model,\n",
    "                   'params': params, \n",
    "                   'results': rfc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to disk\n",
    "with open('cv_results.pkl', 'wb') as f:\n",
    "    pickle.dump(cv_results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hlp.summarize_model_performance(cv_results['RandomForestClassifier-Unweighted']['results'])\n",
    "cv_results['RandomForestClassifier-Unweighted']['results']['best_params_inner_cv']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier - Weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attempt sample weights with by downweighting probabilities closer to 0.5\n",
    "weights = np.abs(df_train_dev['outcome_generative_model']-0.5)\n",
    "\n",
    "rfc_weighted = hlp.nested_cv(X_train, y_train_preds, model, params, sample_weight=weights, \n",
    "                             k_outer=K_o, k_inner=K_i, verbosity=verbosity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'RandomForestClassifier-Weighted'\n",
    "cv_results[name] = {'model': model,\n",
    "                   'params': params, \n",
    "                   'results': rfc_weighted}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cv_results.pkl', 'wb') as f:\n",
    "    pickle.dump(cv_results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hlp.summarize_model_performance(cv_results['RandomForestClassifier-Weighted']['results'])\n",
    "cv_results['RandomForestClassifier-Weighted']['results']['best_params_inner_cv']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLPClassifier(max_iter=250, solver='adam')\n",
    "params = [{'hidden_layer_sizes': [(10,30,10), (10,30,30,30,10), (25,50,50,50,25), (20,)],\n",
    "           'activation': ['tanh', 'relu'],\n",
    "           'alpha': [0.0001, 0.05],\n",
    "           'learning_rate': ['constant'],#, 'adaptive'],\n",
    "           'random_state': [123, 987]}\n",
    "         ]\n",
    "\n",
    "nnc = hlp.nested_cv(X_train, y_train_preds, model, params, \n",
    "                    k_outer=K_o, k_inner=K_i, verbosity=verbosity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'NeuralNetworkClassifier'\n",
    "cv_results[name] = {'model': model,\n",
    "                   'params': params, \n",
    "                   'results': nnc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hlp.summarize_model_performance(nnc)\n",
    "nnc['best_params_inner_cv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cv_results.pkl', 'wb') as f:\n",
    "    pickle.dump(cv_results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestRegressor()\n",
    "params = [{'n_estimators': [250],\n",
    "           'max_depth': [None, 5, 10, 50],\n",
    "           'max_features': [None, 'sqrt', 'log2']}]\n",
    "\n",
    "rfr = hlp.nested_cv(X_train, np.log(y_train_probs), model, params, tune_metric='neg_mean_squared_error',\n",
    "                    k_outer=K_o, k_inner=K_i, verbosity=verbosity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'RandomForestRegressor'\n",
    "cv_results[name] = {'model': model,\n",
    "                   'params': params, \n",
    "                   'results': rfr}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hlp.summarize_model_performance(cv_results['RandomForestRegressor']['results'])\n",
    "cv_results['RandomForestRegressor']['results']['best_params_inner_cv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cv_results.pkl', 'wb') as f:\n",
    "    pickle.dump(cv_results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLPRegressor(max_iter=300, solver='adam') # increased iter d/t convergence issues\n",
    "\n",
    "params = [{'hidden_layer_sizes': [(10,30,10), (10,30,30,30,10), (20,)],\n",
    "           'activation': ['tanh', 'relu'],\n",
    "           'alpha': [0.0001, 0.05],\n",
    "           #'learning_rate': ['constant'],#, 'adaptive'], # only used with 'sgd' solver\n",
    "           'random_state': [123, 987]}\n",
    "         ]\n",
    "\n",
    "nnr = hlp.nested_cv(X_train, np.log(y_train_probs), model, params, tune_metric='neg_mean_squared_error',\n",
    "                    k_outer=K_o, k_inner=K_i, verbosity=verbosity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'NeuralNetworkRegressor'\n",
    "cv_results[name] = {'model': model,\n",
    "                   'params': params, \n",
    "                   'results': nnr}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hlp.summarize_model_performance(cv_results['NeuralNetworkRegressor']['results'])\n",
    "cv_results['NeuralNetworkRegressor']['results']['best_params_inner_cv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cv_results.pkl', 'wb') as f:\n",
    "    pickle.dump(cv_results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nested CV Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for score in ['acc', 'f1', 'mse', 'auc']:\n",
    "    results_partial = []\n",
    "    for name in cv_results.keys():\n",
    "        results_partial.append(cv_results[name]['results'][score])\n",
    "    fig = plt.figure()\n",
    "    fig.suptitle('Algorithm Comparison based on ' + score)\n",
    "    ax = fig.add_subplot(111)\n",
    "    plt.boxplot(results_partial)\n",
    "    plt.ylabel(score)\n",
    "    ax.set_xticklabels(cv_results.keys())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, the classifiers did better than the regressors based on F1 & AUC. The MSE & Accuracy aren't too much better than random guessing given the low event rate. It also appears the random forests perform better than the neural networks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare final figures for thesis\n",
    "cv_results = pickle.load(open('./cv_results.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_partial = []\n",
    "for name in cv_results.keys():\n",
    "    results_partial.append(cv_results[name]['results']['f1'])\n",
    "fig = plt.figure(figsize=(16, 5))\n",
    "fig.suptitle('Algorithm Comparison based on F1 Score')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(results_partial)\n",
    "plt.ylabel('F1')\n",
    "ax.set_xticklabels(cv_results.keys())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_partial = []\n",
    "for name in cv_results.keys():\n",
    "    results_partial.append(cv_results[name]['results']['auc'])\n",
    "fig = plt.figure(figsize=(16, 5))\n",
    "fig.suptitle('Algorithm Comparison based on AUROC')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(results_partial)\n",
    "plt.ylabel('AUROC')\n",
    "ax.set_xticklabels(cv_results.keys())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_partial = []\n",
    "for name in cv_results.keys():\n",
    "    results_partial.append(cv_results[name]['results']['mse'])\n",
    "fig = plt.figure(figsize=(16, 5))\n",
    "fig.suptitle('Algorithm Comparison based on Mean Squared Error')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(results_partial)\n",
    "plt.ylabel('Mean Squared Error')\n",
    "ax.set_xticklabels(cv_results.keys())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build \"Best\" Models on Full Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cv_results['RandomForestClassifier-Weighted']['results']['best_params_inner_cv']\n",
    "# class_weight={0: 0.99, 1: 0.01}, max_depth=50, max_features=None, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cv_results['RandomForestClassifier-Unweighted']['results']['best_params_inner_cv']\n",
    "# class_weight=None, max_depth=10, max_features=None, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cv_results['RandomForestRegressor']['results']['best_params_inner_cv']\n",
    "# max_depth=10, max_features=None, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cv_results['NeuralNetworkClassifier']['results']['best_params_inner_cv']\n",
    "# activation='tanh', alpha=0.0001, hidden_layer_sizes=(10, 30, 10), learning_rate='constant', random_state=123,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cv_results['NeuralNetworkRegressor']['results']['best_params_inner_cv']\n",
    "# activation='tanh', alpha=0.05, hidden_layer_sizes=(10, 30, 30, 30, 10), random_state=987, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_rfcw = RandomForestClassifier(n_estimators=1000, random_state=SEED, \n",
    "                                   class_weight={0: 0.99, 1: 0.01}, max_depth=50, max_features=None)\n",
    "\n",
    "weights = np.abs(df_train_dev['outcome_generative_model']-0.5)\n",
    "best_rfcw.fit(X_train, y_train_preds, sample_weight=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_rfcu = RandomForestClassifier(n_estimators=1000, random_state=SEED, \n",
    "                                   class_weight=None, max_depth=10, max_features=None)\n",
    "best_rfcu.fit(X_train, y_train_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_rfr = RandomForestRegressor(n_estimators=1000, random_state=SEED,\n",
    "                                max_depth=10, max_features=None)\n",
    "best_rfr.fit(X_train, np.log(y_train_probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_nnc = MLPClassifier(max_iter=10000, solver='adam', \n",
    "                        activation='tanh', alpha=0.0001, hidden_layer_sizes=(10, 30, 10),\n",
    "                        learning_rate='constant', random_state=123)\n",
    "best_nnc.fit(X_train, y_train_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_nnr = MLPRegressor(max_iter=10000, solver='adam', \n",
    "                       activation='tanh', alpha=0.05, hidden_layer_sizes=(10, 30, 30, 30, 10), random_state=987)\n",
    "best_nnr.fit(X_train, np.log(y_train_probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = best_rfcw.predict(X_train)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.unique(y_pred)) > 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance of Best Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Set Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performance in training set (should be highly fit)\n",
    "# classifiers\n",
    "for model in [best_rfcw, best_rfcu, best_nnc]:\n",
    "    y_pred = model.predict(X_train)\n",
    "    print(model)\n",
    "    print(metrics.classification_report(y_train_preds, y_pred, digits=3))\n",
    "    print(metrics.roc_auc_score(y_train_preds, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regressors\n",
    "for model in [best_rfr, best_nnr]:\n",
    "    y_pred = model.predict(X_train)\n",
    "    y_pred = np.exp(y_pred)\n",
    "    y_pred = np.where(y_pred >= 0.5, 1, 0)\n",
    "    print(model)\n",
    "    print(metrics.classification_report(y_train_preds, y_pred, digits=3))\n",
    "    print(metrics.roc_auc_score(y_train_preds, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Set Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take all 5 \"best\" models & evaluate in the Validation set. The winner can be used to re-build using the combination of Training/Development/Validation sets. Then, evaluate in Test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifiers\n",
    "for model in [best_rfcw, best_rfcu, best_nnc]:\n",
    "    y_pred = model.predict(X_valid)\n",
    "    print(model)\n",
    "    print(metrics.classification_report(Y_valid, y_pred, digits=3))\n",
    "    print(metrics.roc_auc_score(Y_valid, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best classifier = weighted random forest based on F1, Accuracy, & AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regressors\n",
    "for model in [best_rfr, best_nnr]:\n",
    "    y_pred = model.predict(X_valid)\n",
    "    y_pred = np.exp(y_pred)\n",
    "    y_pred = np.where(y_pred >= 0.5, 1, 0)\n",
    "    print(model)\n",
    "    print(metrics.classification_report(Y_valid, y_pred, digits=3))\n",
    "    print(metrics.roc_auc_score(Y_valid, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best regressor = neural network. However, it performed really poorly in the training set. That makes me worried that perhaps it's a bit random, and it makes me want to lean toward using the random forest classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy of random guess in Validation set\n",
    "1-sum(Y_valid)/len(Y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probability distribution from weighted random forest classifier \n",
    "y_pred_proba = best_rfcw.predict_proba(X_valid)[:,CASE]\n",
    "\n",
    "eval = pd.DataFrame({'predicted': np.round(y_pred_proba, 2), \n",
    "                     'actual': np.where(Y_valid==0, 'Control', 'Case')})\n",
    "eval = eval.sort_values(by=['predicted', 'actual'])\n",
    "eval = eval.assign(counts =eval.groupby(['predicted']).cumcount())\n",
    "\n",
    "fig = sns.scatterplot(data=eval, x=\"predicted\", y=\"counts\", \n",
    "                      hue=eval[\"actual\"].tolist(), palette=\"colorblind\", s=100)\n",
    "plt.title('Discriminative Model Performance in Validation Set (F1=0.80, AUC=0.92)')\n",
    "plt.ylabel('Visit Counts (n)')\n",
    "plt.ylim([-1, 40])\n",
    "plt.xlabel('Probability(OIRD)')\n",
    "plt.legend(loc='upper center')\n",
    "sns.set(rc={'figure.figsize': (15, 5), 'figure.dpi': 300})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# important features from random forest\n",
    "top_n = 10\n",
    "importances = best_rfcw.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in best_rfcw.estimators_],\n",
    "             axis=0)\n",
    "indices = np.argsort(importances)[::-1][:top_n]\n",
    "\n",
    "print(\"Feature ranking:\")\n",
    "for f in range(top_n):\n",
    "    print(\"%d. %s (%f)\" % (f + 1, cols[indices[f]], importances[indices[f]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias Correction  \n",
    "\n",
    "Because the event rate of OIRD in the training/dev set on which the models were built is so much lower than the validation set, it might be worth considering a bias correction. However, the test set should have an event rate closer to the training/dev, so I've decided we don't actually need to do this step. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual Review of Misclassifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba = best_rfcw.predict_proba(X_valid)[:,CASE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_valid = df_valid.copy()\n",
    "review_valid['final_model_prob'] = y_pred_proba\n",
    "\n",
    "# subset to those with probability >= 0.5 but labeled as control on manual review\n",
    "review_valid = review_valid[(review_valid['final_model_prob'] >= 0.5) & \\\n",
    "                            (review_valid['label']=='control')] \\\n",
    "                            .sort_values('final_model_prob', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(review_valid.shape[0]):\n",
    "    print(review_valid['visit_occurrence_id'].iloc[i])\n",
    "    print(review_valid['final_model_prob'].iloc[i])\n",
    "    print(review_valid['review_notes'].iloc[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-Fit \"Best\" Model All Data (except Test Set)\n",
    "\n",
    "Based on the performance in the Validation Set, the Weighted Random Forest Classification model performed best. We are re-building the model with all of the data except for the Test Set.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maj_probs_train = majority_model.predict_proba(L=L_train_dev)[:, CASE]\n",
    "maj_probs_valid = majority_model.predict_proba(L=L_valid)[:, CASE]\n",
    "\n",
    "df_train_dev['majority_model_label'] = maj_probs_train\n",
    "df_valid['majority_model_label'] = maj_probs_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the train/dev set has an \"outcome_generative_model\" column that is used for creating\n",
    "#   weights in the weighted RF model - replicating that in the validation set before merging\n",
    "df_valid_temp = df_valid.copy()\n",
    "df_valid_temp['outcome_generative_model'] = y_valid_probs[:, CASE]\n",
    "\n",
    "# merge train/dev and validation sets \n",
    "df_final = df_train_dev.append(df_valid_temp, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update the 'outcome' column now that validation is also there\n",
    "df_final['outcome'] = np.where(pd.isnull(df_final['outcome']), # if label missing...\n",
    "                                  # pull from manual 'label' (same as above code)\n",
    "                                  np.where(df_final['label']=='case', upper_limit, lower_limit), \n",
    "                                  # otherwise, keep it what it is\n",
    "                                  df_final['outcome'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create y variables\n",
    "y_final_probs = np.array(df_final['outcome'])\n",
    "y_final_preds = np.where(df_final['outcome'] >= 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare features - code taken from above\n",
    "\n",
    "# subset columns\n",
    "X_final = df_final[cols]\n",
    "\n",
    "# also, some of the \"counts\" variables didn't have any results because those patients didn't have charts\n",
    "#    consider imputing \"0\" here, too\n",
    "X_final.fillna(0, inplace=True)\n",
    "\n",
    "# scale data \n",
    "X_final = sc.transform(X_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store weights\n",
    "weights_final = np.abs(df_final['outcome_generative_model']-0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_final = RandomForestClassifier(n_estimators=1000, random_state=SEED, \n",
    "                                     class_weight={0: 0.99, 1: 0.01}, max_depth=50, max_features=None)\n",
    "\n",
    "model_final.fit(X_final, y_final_preds, sample_weight=weights_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performance in training set (should be highly fit)\n",
    "y_pred = model_final.predict(X_final)\n",
    "y_pred_proba_final = model_final.predict_proba(X_final)[:,CASE]\n",
    "print(model_final)\n",
    "print(metrics.classification_report(y_final_preds, y_pred, digits=3))\n",
    "print(metrics.roc_auc_score(y_final_preds, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store predictions on data set & export for prediction model development \n",
    "df_final['snorkel_deterministic_model_prob'] = y_pred_proba_final\n",
    "df_final.to_csv('./train_dev_valid_set_with_predicted_labels.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply Predictions from Final Deterministic Model to Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeating code from above on train/dev and valid sets\n",
    "_, _, _, df_test = hlp.reattach_numeric_data(df_train_dev, df_train_dev, df_valid, df_test)\n",
    "\n",
    "df_test = df_test.merge(naloxone[['visit_occurrence_id', 'binary_naloxone_admin']], \n",
    "                          how='left', on=['visit_occurrence_id'])\n",
    "df_test['binary_respiratory_failure_any'] = \\\n",
    "    np.where(df_test['respiratory_failure_any'].str.contains('1'), 1, 0)\n",
    "df_test['binary_eligible_vent'] = \\\n",
    "    np.where(df_test['eligible_vent'].str.contains('Yes'), 1, 0)\n",
    "df_test['binary_gender_female'] = np.where(df_test['gender']=='FEMALE', 1, 0)\n",
    "df_test = df_test.fillna(value={'binary_naloxone_admin': 0})\n",
    "df_test['binary_cond_resp_failure'] = np.where(df_test['cond_resp_failure']==1, 1, 0)\n",
    "df_test['binary_cond_sepsis'] = np.where(df_test['cond_sepsis']==1, 1, 0)\n",
    "df_test['binary_cond_cva'] = np.where(df_test['cond_cva']==1, 1, 0)\n",
    "df_test['binary_cond_resp_disease'] = np.where(df_test['cond_resp_disease']==1, 1, 0)\n",
    "df_test['binary_cond_cv_disease'] = np.where(df_test['cond_cv_disease']==1, 1, 0)\n",
    "\n",
    "X_test = df_test[cols]\n",
    "X_test.fillna(0, inplace=True)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_binar_test = model_final.predict(X_test)\n",
    "y_pred_proba_test = model_final.predict_proba(X_test)[:,CASE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(y_pred_binar_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(y_pred_binar_test)/len(y_pred_binar_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(y_pred_proba_test, bins=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "non_small = y_pred_proba_test[np.where(y_pred_proba_test > 0.01)]\n",
    "plt.hist(non_small, bins=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep generative model probs for comparison\n",
    "df_test['snorkel_generative_model_prob'] = label_model.predict_proba(L_test)[:, CASE]\n",
    "\n",
    "# store majority model probs\n",
    "df_test['majority_model_label'] = majority_model.predict_proba(L_test)[:, CASE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "df_test['snorkel_deterministic_model_prob'] = y_pred_proba_test\n",
    "df_test.to_csv('./test_set_with_predicted_labels.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manuscript Aids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load updated training/dev/valid data after labeling from previous round\n",
    "df_train, df_dev, df_valid, df_test = hlp.load_data(round=5)\n",
    "\n",
    "# re-attach numeric data to reflect any updated rules\n",
    "df_train, df_dev, df_valid, df_test = hlp.reattach_numeric_data(df_train, df_dev, df_valid, df_test)\n",
    "\n",
    "# keep confounding diagnoses visits available\n",
    "confounding_diagnosis_present = pd.read_csv('../sd_structured/icd/visits_with_confounding_icd_codes.csv')\n",
    "\n",
    "# made changes in code in to loading confounding diagnoses, so eliminating the redundant columns in corrected\n",
    "#    validation set\n",
    "df_valid.drop(['condition_start_date', 'cva', 'sepsis'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_train.shape[0])\n",
    "print(len(np.unique(df_train['grid'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_dev.shape[0])\n",
    "print(len(np.unique(df_dev['grid'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_valid.shape[0])\n",
    "print(len(np.unique(df_valid['grid'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_test.shape[0])\n",
    "print(len(np.unique(df_test['grid'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
