{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import model_selection, metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import helper as hlp \n",
    "import importlib\n",
    "importlib.reload(hlp)\n",
    "\n",
    "# global variables\n",
    "ABSTAIN = -1; CONTROL = 0; CASE = 1\n",
    "SEED = 987"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test set data with snorkel's predictions included\n",
    "df_test = pd.read_csv('./test_set_with_predicted_labels.csv')\n",
    "\n",
    "# including majority vote model for comparisons\n",
    "df_test.rename(columns={\"majority_model_label\": 'snorkel_majority_model_prob'},\n",
    "              inplace=True)\n",
    "\n",
    "# load crowdsourcing determinations\n",
    "crowdsourced = pd.read_csv('./test_set_adjudicated.csv')\n",
    "\n",
    "# merge dataframes\n",
    "merged = crowdsourced.merge(df_test, on=['visit_occurrence_id', 'grid'], how='left')\n",
    "\n",
    "merged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The original test set size included ' + str(df_test.shape[0]) + ' visits.')\n",
    "print('The Crowdsourcing Core reviewed ' + str(crowdsourced.shape[0]) + \\\n",
    "      ' visits for respiratory depression.')\n",
    "print(str(df_test.shape[0] - crowdsourced.shape[0]) + \\\n",
    "      ' visits were excluded in the 1st task of determing non-emergent nature of surgery.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which charts had disagreements with insufficient reviewers?\n",
    "merged[np.isnan(merged['outcome'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# who are the predicted cases?\n",
    "merged[merged['snorkel_deterministic_model_prob'] >= 0.5]['grid'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the original hold-out test set (n=764 visits), 25 visits had Snorkel discriminative model probabilities >= 0.5. Of those 25 visits, only 19 were reviewed for OIRD by the crowdsourcing core (i.e.,6 visits were determined to not be an elective/non-emergent surgery).  \n",
    "\n",
    "Of the 19 reviewed (see GRIDs above), 4 were identified as CASEs by the crowdsourcing core. Alvin reviewed the remaining 15 to see if any could be re-classified - unfortunately, all were controls except for 3 of which 1 was a case & 2 were indeterminate. Further, the single CASE was only reviewed by 2 external reviewers & they disagreed on the determination without a 3rd reviewer to break the tie.  \n",
    "\n",
    "For purposes of this project, instead of removing the high-probability visit due to insufficient reviews, we will classify that visit as a CASE.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually re-classify the single review \n",
    "merged.loc[merged['visit_occurrence_id'] == 41820280, 'outcome'] = 1.0\n",
    "\n",
    "# also specify the 'agreement' as a disagreement\n",
    "merged.loc[merged['visit_occurrence_id'] == 41820280, 'agreement'] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the indeterminate visit\n",
    "merged = merged[~np.isnan(merged['outcome'])]\n",
    "merged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# of the 598 visits, how many unique patients? \n",
    "len(np.unique(merged['grid']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged[merged['outcome']==1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure none of the duplicated patients are associated with visits in both case & control\n",
    "assert merged[merged['grid']=='R286378592'].shape[0] == 1\n",
    "assert merged[merged['grid']=='R285802495'].shape[0] == 1\n",
    "assert merged[merged['grid']=='R284169073'].shape[0] == 1\n",
    "assert merged[merged['grid']=='R262848683'].shape[0] == 1\n",
    "assert merged[merged['grid']=='R206264220'].shape[0] == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export the final determinations for genetic analysis\n",
    "\n",
    "# export GRIDs that were evaluated for OIRD for genetic analysis\n",
    "df = merged.reset_index()['grid']\n",
    "grids_crowdsourced = pd.DataFrame(df)\n",
    "len(np.unique(grids_crowdsourced))\n",
    "\n",
    "# export the cases into phenotype file\n",
    "df = merged[merged['outcome']==1].reset_index()['grid']\n",
    "pheno_crowdsourced = pd.DataFrame(df)\n",
    "pheno_crowdsourced['grid_repeated'] = df\n",
    "\n",
    "# ensure none of the duplicated patients are associated with visits in both case & control\n",
    "assert sum(pheno_crowdsourced['grid'].isin(grids_crowdsourced[grids_crowdsourced.duplicated()]['grid'])) == 0\n",
    "assert sum(grids_crowdsourced[grids_crowdsourced.duplicated()]['grid'].isin(pheno_crowdsourced['grid'])) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot similar to previous evaluation plots\n",
    "for m in ['deterministic', 'generative', 'majority']:\n",
    "    plt.figure()\n",
    "    eval = pd.DataFrame({'predicted': np.round(merged['snorkel_' + m + '_model_prob'], 2), \n",
    "                         'actual': np.where(merged['outcome']==0, 'Control', 'Case')})\n",
    "    eval = eval.sort_values(by=['predicted', 'actual'])\n",
    "    eval = eval.assign(counts =eval.groupby(['predicted']).cumcount())\n",
    "\n",
    "    fig = sns.scatterplot(data=eval, x=\"predicted\", y=\"counts\", \n",
    "                          hue=eval[\"actual\"].tolist(), palette=\"colorblind\", s=100)\n",
    "    plt.ylabel('Counts')\n",
    "    plt.xlabel('Predicted Value')\n",
    "    plt.legend(loc='upper center')\n",
    "    plt.title(m + ' model')\n",
    "    sns.set(rc={'figure.figsize': (15, 5)})\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# because the sample size is larger, it's difficult to see some of the nuances\n",
    "# sub-setting to the non-0 group\n",
    "for m in ['deterministic', 'generative', 'majority']:\n",
    "    plt.figure()\n",
    "    non_zero = merged[merged['snorkel_' + m + '_model_prob'] > 0.]\n",
    "    eval = pd.DataFrame({'predicted': np.round(non_zero['snorkel_' + m + '_model_prob'], 2), \n",
    "                         'actual': np.where(non_zero['outcome']==0, 'Control', 'Case')})\n",
    "    eval = eval.sort_values(by=['predicted', 'actual'])\n",
    "    eval = eval.assign(counts =eval.groupby(['predicted']).cumcount())\n",
    "\n",
    "    fig = sns.scatterplot(data=eval, x=\"predicted\", y=\"counts\", \n",
    "                          hue=eval[\"actual\"].tolist(), palette=\"colorblind\", s=100)\n",
    "    plt.ylabel('Counts')\n",
    "    plt.xlabel('Predicted Value')\n",
    "    plt.legend(loc='upper center')\n",
    "    plt.title(m + ' model with non-zero values')\n",
    "    sns.set(rc={'figure.figsize': (15, 5)})\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# were any of the predicted 0 values actually a case?\n",
    "for m in ['deterministic', 'generative', 'majority']:\n",
    "    print(merged[(merged['snorkel_' + m + '_model_prob'] == 0.) & \\\n",
    "                 (merged['outcome'] == 1.0)].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what were the values when disagreements occurred?\n",
    "merged[merged['agreement'] < 1][['outcome', 'snorkel_deterministic_model_prob', \n",
    "                                 'snorkel_generative_model_prob', 'snorkel_majority_model_prob']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating another plot for when there was disagreement\n",
    "for m in ['deterministic', 'generative', 'majority']:\n",
    "    plt.figure()\n",
    "    eval = pd.DataFrame({'predicted': np.round(merged['snorkel_' + m + '_model_prob'], 2), \n",
    "                         'actual': np.where(merged['agreement']==0, 'Disagreement Present', 'Concensus')})\n",
    "    eval = eval.sort_values(by=['predicted', 'actual'])\n",
    "    eval = eval.assign(counts =eval.groupby(['predicted']).cumcount())\n",
    "\n",
    "    fig = sns.scatterplot(data=eval, x=\"predicted\", y=\"counts\", \n",
    "                          hue=eval[\"actual\"].tolist(), palette=\"colorblind\", s=100)\n",
    "    plt.ylabel('Counts')\n",
    "    plt.xlabel('Predicted Value')\n",
    "    plt.legend(loc='upper center')\n",
    "    plt.title(m + ' model with disagreements')\n",
    "    sns.set(rc={'figure.figsize': (15, 5)})\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# direct comparison of generative  vs. deterministic model\n",
    "colors = np.where(merged['outcome'] == 1.0, 'orange', 'blue')\n",
    "\n",
    "plt.scatter(merged['snorkel_generative_model_prob'], \n",
    "            merged['snorkel_deterministic_model_prob'],\n",
    "            color=colors)\n",
    "plt.xlabel('Generative Model')\n",
    "plt.ylabel('Deterministic Model')\n",
    "plt.title('Comparison of Predicted Probabilities between Generative & Deterministic Models')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how would this plot look if some of the controls were re-classified due to:\n",
    "# disagreement from manual reviewers and/or Alvin's post-hoc designation as 'indeterminate'\n",
    "reclassified = merged.copy()\n",
    "\n",
    "# keep original outcome determination\n",
    "reclassified['outcome_orig'] = reclassified['outcome']\n",
    "\n",
    "# create new categories of outcomes\n",
    "reclassified['outcome'] = np.where(reclassified['outcome'] == 1.0, 'case-crowdsourced', 'control-crowdsourced')\n",
    "reclassified.loc[reclassified['visit_occurrence_id'] == 30427405, 'outcome'] = 'indeterminate-per-alvin'\n",
    "reclassified.loc[reclassified['visit_occurrence_id'] == 46362145, 'outcome'] = 'indeterminate-per-alvin'\n",
    "\n",
    "# create color map for plotting\n",
    "col_map = {'case-crowdsourced': 'orange',\n",
    "          'control-crowdsourced': 'blue',\n",
    "          'indeterminate-per-alvin': 'red'}\n",
    "reclassified['color'] = reclassified['outcome'].map(col_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full agreement for cases & controls\n",
    "rc1a = reclassified[(reclassified['agreement'] == 1.0) & (reclassified['outcome'] == 'case-crowdsourced')]\n",
    "rc1b = reclassified[(reclassified['agreement'] == 1.0) & (reclassified['outcome'] == 'control-crowdsourced')]\n",
    "\n",
    "# some disagreement\n",
    "rc2a = reclassified[(reclassified['agreement'] == 0.0)  & (reclassified['outcome'] == 'case-crowdsourced')]\n",
    "rc2b = reclassified[(reclassified['agreement'] == 0.0)  & (reclassified['outcome'] == 'control-crowdsourced')]\n",
    "\n",
    "# review of top-scoring\n",
    "rc3 = reclassified[reclassified['outcome'] == 'indeterminate-per-alvin']\n",
    "\n",
    "plt.figure(figsize=(10, 10), dpi=150)\n",
    "\n",
    "plt.scatter(rc1a['snorkel_generative_model_prob'], rc1a['snorkel_deterministic_model_prob'],\n",
    "            color='red', marker='^', label='Full Agreement for CASE')\n",
    "plt.scatter(rc1b['snorkel_generative_model_prob'], rc1b['snorkel_deterministic_model_prob'],\n",
    "            color='blue', marker='.', label='Full Agreement CONTROL')\n",
    "\n",
    "plt.scatter(rc2a['snorkel_generative_model_prob'], rc2a['snorkel_deterministic_model_prob'],\n",
    "            color='red', marker='^', facecolors='none', label='Some Disagreement but determined CASE')\n",
    "plt.scatter(rc2b['snorkel_generative_model_prob'], rc2b['snorkel_deterministic_model_prob'],\n",
    "            color='blue', marker='v', facecolors='none',label='Some Disagreement but determined CONTROL')\n",
    "\n",
    "plt.scatter(rc3['snorkel_generative_model_prob'], rc3['snorkel_deterministic_model_prob'],\n",
    "            color='orange', marker='>', facecolors='none', label='Indeterminate')\n",
    "\n",
    "# add line to show potential thresholds with gen > 0.8 and det > 0.7\n",
    "#plt.plot([0.8, 1.05], [0.7, 0.7], color='red', linestyle='-', linewidth=2)\n",
    "#plt.plot([0.8, 0.8], [0.7, 1.05], color='red', linestyle='-', linewidth=2)\n",
    "\n",
    "plt.xlabel('Probability(OIRD) - Generative Model')\n",
    "plt.ylabel('Probability(OIRD) - Discriminative Model')\n",
    "plt.legend(loc='upper left')\n",
    "plt.title('Comparison of Predicted Probabilities between Generative & Discriminative Models')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full agreement controls removed\n",
    "plt.figure(figsize=(10, 10), dpi=150)\n",
    "\n",
    "plt.scatter(rc1a['snorkel_generative_model_prob'], rc1a['snorkel_deterministic_model_prob'],\n",
    "            color='red', marker='^', label='Full Agreement for CASE')\n",
    "#plt.scatter(rc1b['snorkel_generative_model_prob'], rc1b['snorkel_deterministic_model_prob'],\n",
    "#            color='blue', marker='.', label='Full Agreement CONTROL')\n",
    "\n",
    "plt.scatter(rc2a['snorkel_generative_model_prob'], rc2a['snorkel_deterministic_model_prob'],\n",
    "            color='red', marker='^', facecolors='none', label='Some Disagreement but determined CASE')\n",
    "plt.scatter(rc2b['snorkel_generative_model_prob'], rc2b['snorkel_deterministic_model_prob'],\n",
    "            color='blue', marker='v', facecolors='none',label='Some Disagreement but determined CONTROL')\n",
    "\n",
    "plt.scatter(rc3['snorkel_generative_model_prob'], rc3['snorkel_deterministic_model_prob'],\n",
    "            color='orange', marker='>', facecolors='none', label='Indeterminate')\n",
    "\n",
    "plt.xlabel('Probability(OIRD) - Generative Model')\n",
    "plt.ylabel('Probability(OIRD) - Discriminative Model')\n",
    "plt.legend(loc='upper left')\n",
    "plt.title('Comparison of Predicted Probabilities between Generative & Discriminative Models' + \n",
    "          ' (Full Agreement Controls Removed)')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in ['deterministic', 'generative', 'majority']:\n",
    "    plt.figure()\n",
    "    sns.stripplot('outcome', 'snorkel_' + m + '_model_prob', data=merged, jitter=0.2)\n",
    "    plt.title(m + ' model')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# auc curve\n",
    "for m in ['deterministic', 'generative', 'majority']:\n",
    "    plt.figure()\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(merged['outcome'].tolist(), \n",
    "                                             merged['snorkel_' + m + '_model_prob'].tolist())\n",
    "    plt.plot(fpr,tpr, label = 'Crowdsourced')\n",
    "    fs=15\n",
    "    plt.xlabel(\"False Positive Rate\", fontsize=fs)\n",
    "    plt.ylabel(\"True Positive Rate\", fontsize=fs)\n",
    "    plt.tick_params(labelsize=fs-3)\n",
    "    plt.legend(loc='upper left', fontsize=fs-2, title='Test Set Performance')\n",
    "    plt.title(m + ' model AUC')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recall precision curve\n",
    "for m in ['deterministic', 'generative', 'majority']:\n",
    "    plt.figure()\n",
    "    ppv, sens, thresh = metrics.precision_recall_curve(merged['outcome'].tolist(), \n",
    "                                                       merged['snorkel_' + m + '_model_prob'].tolist())\n",
    "    plt.plot(sens, ppv, label = 'Crowdsourced') # cui difference\n",
    "    fs=15\n",
    "    plt.xlabel(\"Recall\", fontsize=fs)\n",
    "    plt.ylabel(\"Precision\", fontsize=fs)\n",
    "    plt.xlim(0, 1.1)\n",
    "    plt.ylim(0, 1.1)\n",
    "    plt.tick_params(labelsize=fs-3)\n",
    "    plt.legend(loc='upper right', fontsize=fs-2, title='Test Set Performance')\n",
    "    plt.title(m + ' model F1')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for thesis document\n",
    "plt.figure()\n",
    "ppv, sens, thresh = metrics.precision_recall_curve(merged['outcome'].tolist(), \n",
    "                                                   merged['snorkel_generative_model_prob'].tolist())\n",
    "plt.plot(sens, ppv, label = 'Crowdsourced') \n",
    "fs=15\n",
    "plt.xlabel(\"Recall\", fontsize=fs)\n",
    "plt.ylabel(\"Precision\", fontsize=fs)\n",
    "plt.xlim(0, 1.1)\n",
    "plt.ylim(0, 1.1)\n",
    "plt.tick_params(labelsize=fs-3)\n",
    "#plt.legend(loc='upper right', fontsize=fs-2, title='Model')\n",
    "plt.title('Recall-Precision Curve for Generative Model in Test Set (F1=0.417)',\n",
    "         fontsize=fs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "ppv, sens, thresh = metrics.precision_recall_curve(merged['outcome'].tolist(), \n",
    "                                                   merged['snorkel_deterministic_model_prob'].tolist())\n",
    "plt.plot(sens, ppv, label = 'Crowdsourced') \n",
    "fs=15\n",
    "plt.xlabel(\"Recall\", fontsize=fs)\n",
    "plt.ylabel(\"Precision\", fontsize=fs)\n",
    "plt.xlim(0, 1.1)\n",
    "plt.ylim(0, 1.1)\n",
    "plt.tick_params(labelsize=fs-3)\n",
    "#plt.legend(loc='upper right', fontsize=fs-2, title='Model')\n",
    "plt.title('Recall-Precision Curve for Discriminative Model in Test Set (F1=0.417)',\n",
    "         fontsize=fs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "ppv, sens, thresh = metrics.precision_recall_curve(merged['outcome'].tolist(), \n",
    "                                                   merged['snorkel_majority_model_prob'].tolist())\n",
    "plt.plot(sens, ppv, label = 'Crowdsourced') \n",
    "fs=15\n",
    "plt.xlabel(\"Recall\", fontsize=fs)\n",
    "plt.ylabel(\"Precision\", fontsize=fs)\n",
    "plt.xlim(0, 1.1)\n",
    "plt.ylim(0, 1.1)\n",
    "plt.tick_params(labelsize=fs-3)\n",
    "#plt.legend(loc='upper right', fontsize=fs-2, title='Model')\n",
    "plt.title('Recall-Precision Curve for Discriminative Model in Test Set (F1=0.333)',\n",
    "         fontsize=fs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy\n",
    "for t in [0.5, 0.7, 0.8, 0.9]:\n",
    "    for m in ['deterministic', 'generative', 'majority']:\n",
    "        p = metrics.accuracy_score(merged['outcome'], \n",
    "                             np.where(merged['snorkel_' + m + '_model_prob'] >= t, 1, 0))\n",
    "        print(m + ' Accuracy with threshold of ' + str(t) + ' = ' + str(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUC\n",
    "for t in [0.5, 0.7, 0.8, 0.9]:\n",
    "    for m in ['deterministic', 'generative', 'majority']:\n",
    "        p = metrics.roc_auc_score(merged['outcome'], \n",
    "                             np.where(merged['snorkel_' + m + '_model_prob'] >= t, 1, 0))\n",
    "        print(m + ' AUC with threshold of ' + str(t) + ' = ' + str(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F1-score\n",
    "for t in [0.5, 0.7, 0.8, 0.9]:\n",
    "    for m in ['deterministic', 'generative', 'majority']:\n",
    "        p = metrics.f1_score(merged['outcome'], \n",
    "                             np.where(merged['snorkel_' + m + '_model_prob'] >= t, 1, 0))\n",
    "        print(m + ' F1 Score with threshold of ' + str(t) + ' = ' + str(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what if we pulled 2 thresholds from the test set (e.g., see scatter plot above)\n",
    "# using generative > 0.8 and deterministic > 0.7\n",
    "metrics.f1_score(merged['outcome'],\n",
    "                 np.where((merged['snorkel_deterministic_model_prob'] >= 0.7) & \\\n",
    "                          (merged['snorkel_generative_model_prob'] >= 0.8), \n",
    "                          1, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the goal is massive labeling, an F1 score of 0.625 isn't too bad. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sensitivity Analysis  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in ['snorkel_deterministic_model_prob', 'sens_rfc_unweighted_prob',\n",
    "       'sens_fully_generative_unweighted_prob',\n",
    "       'sens_fully_generative_weighted_prob']:\n",
    "    plt.figure()\n",
    "    sns.stripplot('outcome', m, data=merged, jitter=0.2)\n",
    "    plt.title(m + ' model')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rc1a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rc2a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rc2b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rc3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original \n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "plt.scatter(rc1a['snorkel_generative_model_prob'], rc1a['snorkel_deterministic_model_prob'],\n",
    "            color='red', marker='^', label='Full Agreement for CASE')\n",
    "#plt.scatter(rc1b['snorkel_generative_model_prob'], rc1b['snorkel_deterministic_model_prob'],\n",
    "#            color='blue', marker='.', label='Full Agreement CONTROL')\n",
    "\n",
    "plt.scatter(rc2a['snorkel_generative_model_prob'], rc2a['snorkel_deterministic_model_prob'],\n",
    "            color='red', marker='^', facecolors='none', label='Some Disagreement but determined CASE')\n",
    "plt.scatter(rc2b['snorkel_generative_model_prob'], rc2b['snorkel_deterministic_model_prob'],\n",
    "            color='blue', marker='v', facecolors='none',label='Some Disagreement but determined CONTROL')\n",
    "\n",
    "plt.scatter(rc3['snorkel_generative_model_prob'], rc3['snorkel_deterministic_model_prob'],\n",
    "            color='orange', marker='>', facecolors='none', label='Indeterminate')\n",
    "\n",
    "plt.xlabel('Generative Model')\n",
    "plt.ylabel('Discriminative Model')\n",
    "plt.legend(loc='upper left')\n",
    "plt.title('Comparison of Predicted Probabilities between Generative & Discriminative Models' + \n",
    "          ' (Full Agreement Controls Removed)')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generative vs. Unweighted random forest classifier\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "comparator = 'sens_rfc_unweighted_prob'\n",
    "plt.scatter(rc1a['snorkel_generative_model_prob'], rc1a[comparator],\n",
    "            color='red', marker='^', label='Full Agreement for CASE')\n",
    "#plt.scatter(rc1b['snorkel_generative_model_prob'], rc1b[comparator],\n",
    "#            color='blue', marker='.', label='Full Agreement CONTROL')\n",
    "\n",
    "plt.scatter(rc2a['snorkel_generative_model_prob'], rc2a[comparator],\n",
    "            color='red', marker='^', facecolors='none', label='Some Disagreement but determined CASE')\n",
    "plt.scatter(rc2b['snorkel_generative_model_prob'], rc2b[comparator],\n",
    "            color='blue', marker='v', facecolors='none',label='Some Disagreement but determined CONTROL')\n",
    "\n",
    "plt.scatter(rc3['snorkel_generative_model_prob'], rc3[comparator],\n",
    "            color='orange', marker='>', facecolors='none', label='Indeterminate')\n",
    "\n",
    "plt.xlabel('Generative Model')\n",
    "plt.ylabel('Discriminative Model - Unweighted Random Forest')\n",
    "plt.legend(loc='upper left')\n",
    "plt.title('Comparison of Predicted Probabilities between Generative & Discriminative Models' + \n",
    "          ' (Full Agreement Controls Removed)')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generative vs. Fully Generative Labels used when building nweighted random forest classifier\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "comparator = 'sens_fully_generative_unweighted_prob'\n",
    "plt.scatter(rc1a['snorkel_generative_model_prob'], rc1a[comparator],\n",
    "            color='red', marker='^', label='Full Agreement for CASE')\n",
    "#plt.scatter(rc1b['snorkel_generative_model_prob'], rc1b[comparator],\n",
    "#            color='blue', marker='.', label='Full Agreement CONTROL')\n",
    "\n",
    "plt.scatter(rc2a['snorkel_generative_model_prob'], rc2a[comparator],\n",
    "            color='red', marker='^', facecolors='none', label='Some Disagreement but determined CASE')\n",
    "plt.scatter(rc2b['snorkel_generative_model_prob'], rc2b[comparator],\n",
    "            color='blue', marker='v', facecolors='none',label='Some Disagreement but determined CONTROL')\n",
    "\n",
    "plt.scatter(rc3['snorkel_generative_model_prob'], rc3[comparator],\n",
    "            color='orange', marker='>', facecolors='none', label='Indeterminate')\n",
    "\n",
    "plt.xlabel('Generative Model')\n",
    "plt.ylabel('Discriminative Model - Unweighted RF with Fully Generative Labels')\n",
    "plt.legend(loc='upper left')\n",
    "plt.title('Comparison of Predicted Probabilities between Generative & Discriminative Models' + \n",
    "          ' (Full Agreement Controls Removed)')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generative vs. Fully Generative Labels used when building weighted random forest classifier\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "comparator = 'sens_fully_generative_weighted_prob'\n",
    "plt.scatter(rc1a['snorkel_generative_model_prob'], rc1a[comparator],\n",
    "            color='red', marker='^', label='Full Agreement for CASE')\n",
    "#plt.scatter(rc1b['snorkel_generative_model_prob'], rc1b[comparator],\n",
    "#            color='blue', marker='.', label='Full Agreement CONTROL')\n",
    "\n",
    "plt.scatter(rc2a['snorkel_generative_model_prob'], rc2a[comparator],\n",
    "            color='red', marker='^', facecolors='none', label='Some Disagreement but determined CASE')\n",
    "plt.scatter(rc2b['snorkel_generative_model_prob'], rc2b[comparator],\n",
    "            color='blue', marker='v', facecolors='none',label='Some Disagreement but determined CONTROL')\n",
    "\n",
    "plt.scatter(rc3['snorkel_generative_model_prob'], rc3[comparator],\n",
    "            color='orange', marker='>', facecolors='none', label='Indeterminate')\n",
    "\n",
    "plt.xlabel('Generative Model')\n",
    "plt.ylabel('Discriminative Model - Weighted RF with Fully Generative Labels')\n",
    "plt.legend(loc='upper left')\n",
    "plt.title('Comparison of Predicted Probabilities between Generative & Discriminative Models' + \n",
    "          ' (Full Agreement Controls Removed)')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(merged['snorkel_deterministic_model_prob'], merged['sens_rfc_unweighted_prob'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "['snorkel_deterministic_model_prob', 'sens_rfc_unweighted_prob',\n",
    "       'sens_fully_generative_unweighted_prob',\n",
    "       'sens_fully_generative_weighted_prob']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# review of non-test set\n",
    "df_train = pd.read_csv('./train_dev_valid_set_with_predicted_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discrepancies = df_train[['visit_occurrence_id', 'label', 'outcome_generative_model', \n",
    "                          'snorkel_deterministic_model_prob', 'sens_rfc_unweighted_prob',\n",
    "                          'sens_fully_generative_unweighted_prob', 'sens_fully_generative_weighted_prob']].copy()\n",
    "discrepancies = discrepancies[(np.abs(discrepancies['snorkel_deterministic_model_prob'] - \n",
    "                               discrepancies['outcome_generative_model']) > 0.2)]\n",
    "discrepancies.sort_values('outcome_generative_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When looking at the actual values of various models, those with really high generative model values that were lowered in the discriminative model occurred in the cases where the model was trained with incorporation of manually adjudicated labels rather than the fully-generative labels. I thought it would've been the weighting that did this, but it seems here that it was actually having some manually-adjudicated data in the mix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dig into the validation set a bit more to see if that was the case...\n",
    "with open('./data_for_analysis.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "df_valid = data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discrepancies[discrepancies['visit_occurrence_id'].isin(df_valid['visit_occurrence_id'])] \\\n",
    "    .sort_values('outcome_generative_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, it looks like there was plenty of information in these hand labels for the validation set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AHRQ PSI-11 Performance  \n",
    "\n",
    "How well did the original criteria we used perform compared to the SNORKEL model? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many cases in AHRQ? \n",
    "merged[merged['study_group']=='case'].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# of the AHRQ cases, how many had a true outcome of 0\n",
    "merged[(merged['study_group']=='case') & (merged['outcome']==1.0)].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat with controls\n",
    "merged[merged['study_group']=='control'].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged[(merged['study_group']=='control') & (merged['outcome']==1.0)].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Performances for Written Thesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure code is working by producing variation\n",
    "for m in ['snorkel_generative_model_prob', 'snorkel_deterministic_model_prob', \n",
    "          'sens_rfc_unweighted_prob', 'sens_fully_generative_weighted_prob', \n",
    "          'sens_fully_generative_unweighted_prob']:\n",
    "    print(np.mean(merged[m]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy    \n",
    "for m in ['snorkel_generative_model_prob', 'snorkel_deterministic_model_prob', \n",
    "          'sens_rfc_unweighted_prob', 'sens_fully_generative_weighted_prob', \n",
    "          'sens_fully_generative_unweighted_prob']:\n",
    "    p = metrics.accuracy_score(merged['outcome'], \n",
    "                               np.where(merged[m] >= 0.5, 1, 0))\n",
    "    print(m + ' Accuracy ' + ' = ' + str(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUC    \n",
    "for m in ['snorkel_generative_model_prob', 'snorkel_deterministic_model_prob', \n",
    "          'sens_rfc_unweighted_prob', 'sens_fully_generative_weighted_prob', \n",
    "          'sens_fully_generative_unweighted_prob']:\n",
    "    p = metrics.roc_auc_score(merged['outcome'], \n",
    "                               np.where(merged[m] >= 0.5, 1, 0))\n",
    "    print(m + ' AUC ' + ' = ' + str(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F1    \n",
    "for m in ['snorkel_generative_model_prob', 'snorkel_deterministic_model_prob', \n",
    "          'sens_rfc_unweighted_prob', 'sens_fully_generative_weighted_prob', \n",
    "          'sens_fully_generative_unweighted_prob']:\n",
    "    p = metrics.f1_score(merged['outcome'], \n",
    "                               np.where(merged[m] >= 0.5, 1, 0))\n",
    "    print(m + ' F1 Score ' + ' = ' + str(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPV\n",
    "for m in ['snorkel_generative_model_prob', 'snorkel_deterministic_model_prob', \n",
    "          'sens_rfc_unweighted_prob', 'sens_fully_generative_weighted_prob', \n",
    "          'sens_fully_generative_unweighted_prob']:\n",
    "    p = metrics.precision_score(merged['outcome'], \n",
    "                               np.where(merged[m] >= 0.5, 1, 0))\n",
    "    print(m + ' PPV ' + ' = ' + str(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sensitivity\n",
    "for m in ['snorkel_generative_model_prob', 'snorkel_deterministic_model_prob', \n",
    "          'sens_rfc_unweighted_prob', 'sens_fully_generative_weighted_prob', \n",
    "          'sens_fully_generative_unweighted_prob']:\n",
    "    p = metrics.recall_score(merged['outcome'], \n",
    "                               np.where(merged[m] >= 0.5, 1, 0))\n",
    "    print(m + ' Sensitivity ' + ' = ' + str(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specificity\n",
    "for m in ['snorkel_generative_model_prob', 'snorkel_deterministic_model_prob', \n",
    "          'sens_rfc_unweighted_prob', 'sens_fully_generative_weighted_prob', \n",
    "          'sens_fully_generative_unweighted_prob']:\n",
    "    p = metrics.classification_report(merged['outcome'], \n",
    "                               np.where(merged[m] >= 0.5, 1, 0))\n",
    "    print(m + ' Specificity (recall of negative class) ' + ' = \\n' + str(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure all these false positive counts are true\n",
    "merged[(merged['snorkel_generative_model_prob'] >= 0.5) & \n",
    "       (merged['outcome']==0.0)].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged[(merged['snorkel_deterministic_model_prob'] >= 0.5) & \n",
    "       (merged['outcome']==0.0)].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged[(merged['sens_rfc_unweighted_prob'] >= 0.5) & \n",
    "       (merged['outcome']==0.0)].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged[(merged['sens_fully_generative_weighted_prob'] >= 0.5) & \n",
    "       (merged['outcome']==0.0)].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged[(merged['sens_fully_generative_unweighted_prob'] >= 0.5) & \n",
    "       (merged['outcome']==0.0)].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AHRQ performance\n",
    "metrics.accuracy_score(merged['outcome'], \n",
    "                       np.where(merged['study_group'] == 'case', 1, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.roc_auc_score(merged['outcome'], \n",
    "                       np.where(merged['study_group'] == 'case', 1, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.f1_score(merged['outcome'], \n",
    "                       np.where(merged['study_group'] == 'case', 1, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.precision_score(merged['outcome'], \n",
    "                       np.where(merged['study_group'] == 'case', 1, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.recall_score(merged['outcome'], \n",
    "                       np.where(merged['study_group'] == 'case', 1, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(merged['outcome'], \n",
    "                       np.where(merged['study_group'] == 'case', 1, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# majority vote model performance\n",
    "metrics.accuracy_score(merged['outcome'], \n",
    "                       np.where(merged['snorkel_majority_model_prob'] >= 0.5, 1, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.roc_auc_score(merged['outcome'], \n",
    "                       np.where(merged['snorkel_majority_model_prob'] >= 0.5, 1, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.f1_score(merged['outcome'], \n",
    "                       np.where(merged['snorkel_majority_model_prob'] >= 0.5, 1, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.precision_score(merged['outcome'], \n",
    "                       np.where(merged['snorkel_majority_model_prob'] >= 0.5, 1, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.recall_score(merged['outcome'], \n",
    "                       np.where(merged['snorkel_majority_model_prob'] >= 0.5, 1, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(merged['outcome'], \n",
    "                       np.where(merged['snorkel_majority_model_prob'] >= 0.5, 1, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
