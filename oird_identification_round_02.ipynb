{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SnorkelMED - Identifying Opioid-Induced Respiratory Depression  \n",
    "\n",
    "The purpose of this analysis is to probabilistically identify which patient visits included an opioid-induced respiratory depression (OIRD) event. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from snorkel.labeling import labeling_function, PandasLFApplier, LFAnalysis\n",
    "from snorkel.labeling.model import MajorityLabelVoter, LabelModel\n",
    "from snorkel.analysis import get_label_buckets\n",
    "\n",
    "majority_model = MajorityLabelVoter()\n",
    "label_model = LabelModel(cardinality=2, verbose=True)\n",
    "\n",
    "import helper as hlp\n",
    "import importlib\n",
    "importlib.reload(hlp)\n",
    "\n",
    "# global variables\n",
    "ABSTAIN = -1; CONTROL = 0; CASE = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load updated training/dev/valid data after labeling from previous round\n",
    "df_train, df_dev, df_valid, df_test = hlp.load_data(round=2)\n",
    "\n",
    "# re-attach numeric data to reflect any updated rules\n",
    "df_train, df_dev = hlp.reattach_numeric_data(df_train, df_dev)\n",
    "\n",
    "# keep confounding diagnoses visits available\n",
    "confounding_diagnosis_present = pd.read_csv('../sd_structured/icd/visits_with_confounding_icd_codes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given the previous changes to excluding persons who are also in the test set,\n",
    "#   ensure no persons in the test set are in the train, valid, or dev sets\n",
    "assert sum(df_train['person_id'].isin(df_test['person_id'])) == 0\n",
    "assert sum(df_dev['person_id'].isin(df_test['person_id'])) == 0\n",
    "assert sum(df_valid['person_id'].isin(df_test['person_id'])) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store Y values for ease of evaluation\n",
    "Y_dev = df_dev['label'].values\n",
    "Y_dev = np.where(Y_dev=='case', 1, 0) \n",
    "\n",
    "#Y_valid = df_valid['label'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Round 2 - Add More Learning Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionary to keep track of rule names for easier reference later\n",
    "lfd = dict()\n",
    "\n",
    "@labeling_function()\n",
    "def LF_naloxone_admin(x):\n",
    "    if x['naloxone_admin_prob'] >= 0.75:\n",
    "        if x['counts_naloxone_effective'] > 0:\n",
    "            return CASE\n",
    "        elif x['counts_naloxone_NOT_effective'] > 0:\n",
    "            return CONTROL\n",
    "        else:\n",
    "            return CASE\n",
    "    elif x['naloxone_admin_prob'] < 0.75:\n",
    "        return CONTROL\n",
    "    else:\n",
    "        # if missing\n",
    "        return ABSTAIN\n",
    "lfd['LF_naloxone_admin'] = 0\n",
    "    \n",
    "@labeling_function()\n",
    "def LF_respiratory_failure_any(x):\n",
    "    # this ended up being a heavily-weighted rule to lean controls toward cases\n",
    "    # but it has excellent coverage\n",
    "    # so adding logic checks to see if other good rules might suggest non-opioid causes\n",
    "    # I thought snorkel should handle all these interactions, but I'm not seeing that to be the case.\n",
    "    if '1' in x['respiratory_failure_any'].lower(): \n",
    "        # if there is a confounding diagonsis that would make respiratory failure likely\n",
    "        if x['visit_occurrence_id'] in confounding_diagnosis_present['visit_occurrence_id'].unique():\n",
    "            return CONTROL\n",
    "        # or if there is a lack of any other evidence to suggest the patient is a case\n",
    "        elif x['counts_naloxone'] == 0 and \\\n",
    "             np.isnan(x['naloxone_admin_prob']) and \\\n",
    "             x['counts_altered_mental_status'] == 0 and \\\n",
    "             x['counts_narcotic_overdose'] == 0 and \\\n",
    "             x['counts_hypoxia'] == 0 and \\\n",
    "             x['counts_decrease_opioids'] == 0:\n",
    "            return CONTROL\n",
    "        else:\n",
    "            return CASE\n",
    "    return CONTROL\n",
    "lfd['LF_respiratory_failure_any'] = max(lfd.values()) + 1\n",
    "\n",
    "@labeling_function()\n",
    "def LF_eligible_vent(x):\n",
    "    if 'yes' in x['eligible_vent'].lower(): \n",
    "        return CASE\n",
    "    return ABSTAIN\n",
    "lfd['LF_eligible_vent'] = max(lfd.values()) + 1\n",
    "\n",
    "@labeling_function()\n",
    "def LF_counts_naloxone(x):\n",
    "    if x['counts_naloxone'] > 0: \n",
    "        return CASE\n",
    "    return ABSTAIN\n",
    "lfd['LF_counts_naloxone'] = max(lfd.values()) + 1\n",
    "\n",
    "@labeling_function()\n",
    "def LF_counts_resp_care_notes(x):\n",
    "    if x['counts_resp_care_notes'] == 0:\n",
    "        return CONTROL\n",
    "    return ABSTAIN\n",
    "lfd['LF_counts_resp_care_notes'] = max(lfd.values()) + 1\n",
    "\n",
    "#@labeling_function()\n",
    "#def LF_counts_rapid_response(x):\n",
    "    # in Round 2, moving this rule to the competing diagnosis rule\n",
    "#    if x['counts_rapid_response'] == 0:\n",
    "#        return CONTROL\n",
    "#    elif x['counts_rapid_response'] == 1:\n",
    "#        return ABSTAIN\n",
    "#    return CASE\n",
    "#lfd['LF_counts_rapid_response'] = max(lfd.values()) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Round 2 Additions\n",
    "\n",
    "@labeling_function()\n",
    "def LF_counts_no_acute_events(x):\n",
    "    # this ended up being a heavily-weighted rule to lean cases toward controls\n",
    "    # so additional logic checks to see if other \"acute\" things happened during the visit\n",
    "    #   (even if they didn't happen for several days where this was recorded)\n",
    "    if x['counts_no_acute_events'] > 0:\n",
    "        if x['counts_naloxone'] + x['counts_rapid_response'] + x['counts_altered_mental_status'] + \\\n",
    "            x['counts_narcotic_overdose'] + x['counts_hypoxia'] > 0:\n",
    "            return ABSTAIN\n",
    "        return CONTROL\n",
    "    return ABSTAIN\n",
    "lfd['LF_counts_no_acute_events'] = max(lfd.values()) + 1\n",
    "\n",
    "@labeling_function()\n",
    "def LF_counts_altered_mental_status(x):\n",
    "    if x['counts_altered_mental_status'] > 0:\n",
    "        return CASE\n",
    "    return ABSTAIN\n",
    "lfd['LF_counts_altered_mental_status'] = max(lfd.values()) + 1\n",
    "\n",
    "@labeling_function()\n",
    "def LF_counts_narcotic_overdose(x):\n",
    "    if x['counts_narcotic_overdose'] > 0:\n",
    "        return CASE\n",
    "    return ABSTAIN\n",
    "lfd['LF_counts_narcotic_overdose'] = max(lfd.values()) + 1\n",
    "\n",
    "@labeling_function()\n",
    "def LF_counts_hypoxia(x):\n",
    "    if x['counts_hypoxia'] > 0:\n",
    "        return CASE\n",
    "    return ABSTAIN\n",
    "lfd['LF_counts_hypoxia'] = max(lfd.values()) + 1\n",
    "\n",
    "@labeling_function()\n",
    "def LF_counts_decrease_opioids(x):\n",
    "    if x['counts_decrease_opioids'] > 0:\n",
    "        return CASE\n",
    "    return ABSTAIN\n",
    "lfd['LF_counts_decrease_opioids'] = max(lfd.values()) + 1\n",
    "\n",
    "# the following 2 labeling functions have been moved to the \n",
    "#   naloxone_admin probability labeling function\n",
    "#@labeling_function()\n",
    "#def LF_counts_naloxone_effective(x):\n",
    "#    if x['counts_naloxone_effective'] > 0:\n",
    "#        return CASE\n",
    "#    return ABSTAIN\n",
    "#lfd['LF_counts_naloxone_effective'] = max(lfd.values()) + 1\n",
    "\n",
    "#@labeling_function()\n",
    "#def LF_counts_naloxone_NOT_effective(x):\n",
    "#    if x['counts_naloxone_NOT_effective'] > 0:\n",
    "#        return CONTROL\n",
    "#    return ABSTAIN\n",
    "#lfd['LF_counts_naloxone_NOT_effective'] = max(lfd.values()) + 1\n",
    "\n",
    "@labeling_function()\n",
    "def LF_confounding_diagnosis_for_rrt(x):\n",
    "    if x['visit_occurrence_id'] in confounding_diagnosis_present['visit_occurrence_id'].unique(): \n",
    "        return CONTROL\n",
    "    elif x['counts_rapid_response'] > 0:\n",
    "        return CASE\n",
    "    return ABSTAIN\n",
    "lfd['LF_confounding_diagnosis_for_rrt'] = max(lfd.values()) + 1\n",
    "\n",
    "@labeling_function()\n",
    "def LF_extended_vent_time(x):\n",
    "    # if on the vent for 4 or more days\n",
    "    if 'yes;;yes;;yes;;yes' in x['eligible_vent'].lower(): \n",
    "        return CONTROL\n",
    "    return ABSTAIN\n",
    "lfd['LF_extended_vent_time'] = max(lfd.values()) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# combine all relevant LFs\n",
    "lfs = [LF_naloxone_admin,\n",
    "      LF_respiratory_failure_any,\n",
    "      LF_eligible_vent,\n",
    "      LF_counts_naloxone,\n",
    "      LF_counts_resp_care_notes,\n",
    "      #LF_counts_rapid_response,\n",
    "      LF_counts_no_acute_events,\n",
    "      LF_counts_altered_mental_status,\n",
    "      LF_counts_narcotic_overdose,\n",
    "      LF_counts_hypoxia,\n",
    "      LF_counts_decrease_opioids,\n",
    "      #LF_counts_naloxone_effective,\n",
    "      #LF_counts_naloxone_NOT_effective,\n",
    "      LF_confounding_diagnosis_for_rrt,\n",
    "      LF_extended_vent_time,\n",
    "      ]\n",
    "\n",
    "# apply LFs\n",
    "applier = PandasLFApplier(lfs=lfs)\n",
    "L_train = applier.apply(df=df_train)\n",
    "L_dev = applier.apply(df=df_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LFAnalysis(L=L_train, lfs=lfs).lf_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LFAnalysis(L=L_dev, lfs=lfs).lf_summary(Y=Y_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.iloc[L_train[:, lfd['LF_respiratory_failure_any']] == CONTROL].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conflicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buckets = get_label_buckets(L_dev[:, lfd['LF_confounding_diagnosis_for_rrt']], \n",
    "                            L_dev[:, lfd['LF_counts_no_acute_events']])\n",
    "df_dev[['label', 'counts_no_acute_events', 'counts_rapid_response', 'sepsis', 'cva']] \\\n",
    "    .iloc[buckets[(CONTROL, CONTROL)]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`eligible_vent` had low covered & low empirical accuracy, and since the introduction of `extended_vent_time` (which has higher empirical accuracy), I'm throwing out `eligible_vent`  \n",
    "\n",
    "`altered_mental_status` has good coverage but poor empirical accuracy. Particularly now that we have the `confounding_diagnosis_present` rule, we could either remove or even consider incorporating it under the diagnosis rule. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lfs.remove(LF_eligible_vent)\n",
    "lfs.remove(LF_counts_altered_mental_status)\n",
    "lfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "applier = PandasLFApplier(lfs=lfs)\n",
    "L_train = applier.apply(df=df_train)\n",
    "L_dev = applier.apply(df=df_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure the code can run\n",
    "label_model.fit(L_train=L_train, Y_dev = Y_dev, \n",
    "                n_epochs = 4000, lr = 0.004, #l2 = 0.01,\n",
    "                optimizer = 'adamax', lr_scheduler = 'step', #prec_init = 0.7,\n",
    "                log_freq = 100, seed = 987)\n",
    "analysis = LFAnalysis(L=L_train, lfs=lfs).lf_summary(est_weights=label_model.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create empty dataframe to hold learned weights for each rule\n",
    "df_cols = ['n_epochs', 'lr', 'lr_scheduler']\n",
    "df_cols.extend(analysis.index)\n",
    "\n",
    "# specify potential hyperparameters\n",
    "n_epochs = [2000, 4000]\n",
    "lr = [0.001, 0.005, 0.01]\n",
    "lr_scheduler = ['step', 'exponential', 'linear']\n",
    "\n",
    "# tune - in round 2, started adding new seeds also (e.g., 987, 456, & 123)\n",
    "df_tune, df_tune_long = hlp.label_model_tuning(lfs, df_cols, \n",
    "                                               L_train, L_dev, Y_dev, \n",
    "                                               n_epochs, lr, lr_scheduler,\n",
    "                                               seed = 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# review best accuracies among development set\n",
    "df_tune[df_tune['accuracy'] == np.max(df_tune['accuracy'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for scheduler in lr_scheduler:\n",
    "    g = sns.FacetGrid(df_tune_long[df_tune_long['lr_scheduler']==scheduler], \n",
    "                      col='lr', hue='learning_function', col_wrap=3, height=4)\n",
    "    g = (g.map(plt.scatter, 'n_epochs', 'learned_weight')\n",
    "            .add_legend()\n",
    "            .fig.suptitle('Learned Weights Using ' + str(scheduler) + ' Scheduler', \n",
    "                          y=1.05, fontsize=16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_model.fit(L_train=L_train, Y_dev = Y_dev, n_epochs = 2000, lr = 0.005, optimizer = 'adamax', \n",
    "                lr_scheduler = 'exponential', log_freq = 100, seed = 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LFAnalysis(L=L_train, lfs=lfs) \\\n",
    "    .lf_summary(est_weights = label_model.get_weights()) \\\n",
    "    .sort_values(by='Learned Weight', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "majority_acc = majority_model.score(L=L_dev, Y=Y_dev)[\"accuracy\"]\n",
    "print(f\"{'Majority Vote Accuracy:':<25} {majority_acc * 100:.1f}%\")\n",
    "\n",
    "label_model_acc = label_model.score(L=L_dev, Y=Y_dev)[\"accuracy\"]\n",
    "print(f\"{'Label Model Accuracy:':<25} {label_model_acc * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign probabilities from either majority vote or label model\n",
    "#gen_probs_train = majority_model.predict_proba(L=L_train)\n",
    "#gen_probs_dev = majority_model.predict_proba(L=L_dev)\n",
    "\n",
    "gen_probs_train = label_model.predict_proba(L=L_train)\n",
    "gen_probs_dev = label_model.predict_proba(L=L_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hlp.plot_probabilities_histogram(gen_probs_train[:, CASE])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hlp.plot_probabilities_histogram(gen_probs_dev[:, CASE])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_with_probs = df_dev.copy()\n",
    "dev_with_probs['label_model_prob'] = gen_probs_dev[:, CASE]\n",
    "\n",
    "cases_low_prob = dev_with_probs[(dev_with_probs['label']=='case') & \\\n",
    "                                (dev_with_probs['label_model_prob']<0.5)]\n",
    "controls_high_prob = dev_with_probs[(dev_with_probs['label']=='control') & \\\n",
    "                                    (dev_with_probs['label_model_prob']>0.5)]\n",
    "\n",
    "cases_low_prob \\\n",
    "    [['label', 'label_model_prob', 'naloxone_admin_prob', 'eligible_vent', 'respiratory_failure_any',\n",
    "     'counts_naloxone', 'counts_resp_care_notes', 'counts_rapid_response', \n",
    "     'counts_no_acute_events', 'counts_altered_mental_status', 'counts_narcotic_overdose',\n",
    "     'counts_hypoxia','counts_decrease_opioids', 'counts_naloxone_effective', 'counts_naloxone_NOT_effective',\n",
    "     'sepsis', 'cva']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_dev[cases_low_prob.index.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "controls_high_prob \\\n",
    "    [['label', 'label_model_prob', 'naloxone_admin_prob', 'eligible_vent', 'respiratory_failure_any',\n",
    "     'counts_naloxone', 'counts_resp_care_notes', 'counts_rapid_response', \n",
    "     'counts_no_acute_events', 'counts_altered_mental_status', 'counts_narcotic_overdose',\n",
    "     'counts_hypoxia','counts_decrease_opioids', 'counts_naloxone_effective', 'counts_naloxone_NOT_effective',\n",
    "     'sepsis', 'cva']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_dev[controls_high_prob.index.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exploration of associations \n",
    "from matplotlib  import cm\n",
    "\n",
    "df_fig = dev_with_probs\n",
    "colors = ['red' if p=='case' else 'blue' for p in df_fig['label']]\n",
    "plt.scatter(df_fig['naloxone_admin_prob'],\n",
    "            df_fig['label_model_prob'],\n",
    "            c=colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(dev_with_probs[dev_with_probs['label']=='case']['label_model_prob'], bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(dev_with_probs[dev_with_probs['label']=='control']['label_model_prob'], bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, it looks like the dev set is illustrating really nice performance on controls with all probabilities for controls being < 7%. Most of the cases have very high probabilities, but there are still 2 patients in the cases group who have *very* low scores. Notably, when I look at the manual review for those 2 patients, these were not the most confident cases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attach LabelModel predictions to dataframes\n",
    "train_with_probs = df_train.copy()\n",
    "train_with_probs['label_model_prob'] = gen_probs_train[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract top 20 highest probabilities from train set\n",
    "top_probs = train_with_probs.nlargest(n=20, columns='label_model_prob')\n",
    "\n",
    "# send half to the dev set & half to the valid set\n",
    "visits_for_dev = top_probs['visit_occurrence_id'].sample(frac=0.5, random_state=123)\n",
    "visits_for_valid = top_probs[~np.isin(top_probs['visit_occurrence_id'], visits_for_dev)]['visit_occurrence_id']\n",
    "\n",
    "# concatenate the respective training set rows to dev & valid sets\n",
    "df_dev3 = pd.concat([df_dev, df_train[df_train['visit_occurrence_id'].isin(visits_for_dev)]], sort=True)\n",
    "df_valid3 = pd.concat([df_valid, df_train[np.isin(df_train['visit_occurrence_id'], visits_for_valid)]], sort=True)\n",
    "\n",
    "# remove the rows from the training set\n",
    "df_train3 = df_train.drop(top_probs.index)\n",
    "\n",
    "assert df_dev3.shape[0] == df_dev.shape[0] + 0.5*top_probs.shape[0]\n",
    "assert df_valid3.shape[0] == df_valid.shape[0] + 0.5*top_probs.shape[0]\n",
    "assert df_train3.shape[0] == df_train.shape[0] - top_probs.shape[0]\n",
    "assert not np.isin(df_train3['visit_occurrence_id'], df_dev3['visit_occurrence_id']).any()\n",
    "assert not np.isin(df_train3['visit_occurrence_id'], df_valid3['visit_occurrence_id']).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export for manual review\n",
    "#df_train3.to_csv('./train_set3.csv', index=False)\n",
    "#df_dev3.to_csv('./dev_set3.csv', index=False)\n",
    "#df_valid3.to_csv('./valid_set3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
