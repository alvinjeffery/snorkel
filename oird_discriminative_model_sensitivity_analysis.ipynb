{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Purpose:* Questions surfaced about how a discriminative model could perform better than the generative model. Therefore, I'm exploring whether some noise awareness in our models of choice are responsible. This includes testing:  \n",
    "- Purely Generative labels without any information from Development Set adjudication  \n",
    "- A non-weighted random forest classifier so that noise isn't accounted for  \n",
    "\n",
    "This notebook is a duplicate of the original discriminative model notebook with many cells deleted & only a few added.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold, GridSearchCV, cross_validate\n",
    "from sklearn import model_selection, metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "# snorkel\n",
    "from snorkel.labeling import labeling_function, PandasLFApplier, LFAnalysis, filter_unlabeled_dataframe\n",
    "from snorkel.labeling.model import MajorityLabelVoter, LabelModel\n",
    "from snorkel.analysis import get_label_buckets, metric_score\n",
    "from snorkel.utils import probs_to_preds\n",
    "\n",
    "majority_model = MajorityLabelVoter()\n",
    "label_model = LabelModel(cardinality=2, verbose=True)\n",
    "\n",
    "import helper as hlp\n",
    "import importlib\n",
    "importlib.reload(hlp)\n",
    "\n",
    "# global variables\n",
    "ABSTAIN = -1; CONTROL = 0; CASE = 1\n",
    "SEED = 987"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data, which was saved as a tuple...\n",
    "#export_data = (df_train_dev, df_valid, df_test, Y_dev, Y_valid,\n",
    "#               L_train, L_dev, L_train_dev, L_valid,  \n",
    "#               label_model, majority_model, L_test)\n",
    "\n",
    "with open('./data_for_analysis.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "    \n",
    "df_train_dev = data[0]; df_valid = data[1]; df_test = data[2]\n",
    "Y_dev = data[3]; Y_valid = data[4]\n",
    "L_train = data[5]; L_dev = data[6]; L_train_dev = data[7]; L_valid = data[8]\n",
    "label_model = data[9]; majority_model = data[10]\n",
    "L_test = data[11]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Model Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outcome  \n",
    "Although we're primarily depending on the Generative model for labels, we can still leverage our manually adjudicated information for more robust information - something is better than nothing, right? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find observed values from label model probabilities that are closest to 0 or 1 and make \n",
    "#    manually-adjudicated labels slightly closer to 0 or 1, respectively\n",
    "label_model_probs = label_model.predict_proba(L_train_dev)[:, CASE]\n",
    "lower_limit = 0.95 * np.min(label_model_probs)\n",
    "upper_limit = 0.95 * (1-np.max(label_model_probs)) + np.max(label_model_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store on dataframe, using manual adjudication if available\n",
    "df_train_dev['outcome_generative_model'] = label_model_probs\n",
    "df_train_dev['outcome'] = np.where(pd.isnull(df_train_dev['label']), # if label is missing...\n",
    "                                           # use generative model\n",
    "                                           df_train_dev['outcome_generative_model'], \n",
    "                                           # otherwise, use manually-adjudicated label \n",
    "                                           # but with offset for regression-based models\n",
    "                                           np.where(df_train_dev['label']=='case', upper_limit, lower_limit))\n",
    "\n",
    "# create y variables\n",
    "y_train_probs = np.array(df_train_dev['outcome'])\n",
    "y_train_preds = np.where(df_train_dev['outcome'] >= 0.5, 1, 0)\n",
    "\n",
    "y_valid_probs = label_model.predict_proba(L_valid)#[:, CASE] # only used as FYI\n",
    "y_valid_preds = probs_to_preds(y_valid_probs) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FYI: Generative Model Performance on Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval = pd.DataFrame({'predicted': np.round(y_valid_probs[:, CASE], 2), \n",
    "                     'actual': np.where(Y_valid==0, 'Control', 'Case')})\n",
    "eval = eval.sort_values(by=['predicted', 'actual'])\n",
    "eval = eval.assign(counts =eval.groupby(['predicted']).cumcount())\n",
    "\n",
    "fig = sns.scatterplot(data=eval, x=\"predicted\", y=\"counts\", \n",
    "                      hue=eval[\"actual\"].tolist(), palette=\"colorblind\", s=100)\n",
    "plt.ylabel('Counts')\n",
    "plt.xlabel('Predicted Value')\n",
    "plt.legend(loc='upper center')\n",
    "sns.set(rc={'figure.figsize': (15, 5)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(Y_valid, y_valid_preds, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictors  \n",
    "\n",
    "For the deterministic model, we're keeping a generalizable set of features. We could depend on the previously-developed learning functions, but one draw-back is the amount of feature engineering that's put into that. Alternatively, we can start with the raw features, e.g., age, regular expression counts, etc. It might also be unwise to use the `nalxone_admin_prob` value due to it being created with a previous Snorkel model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull in original naloxone administration info & only count \"received\" if \"epic ip admin\" or \"hed\" present\n",
    "naloxone = pd.read_csv('../sd_structured/meds/naloxone/naloxone_exposure_pre.csv', sep='\\t')\n",
    "naloxone.columns = naloxone.columns.str.lower()\n",
    "\n",
    "# collapse all visit day onto a single row\n",
    "SEP = ';;'\n",
    "join_as_strings = lambda x: SEP.join(map(str, x))\n",
    "\n",
    "naloxone = naloxone.groupby(['visit_occurrence_id', 'grid', 'label']) \\\n",
    "    ['x_frequency', 'drug_source_value', 'x_doc_type', 'x_doc_stype'] \\\n",
    "    .agg(join_as_strings) \\\n",
    "    .reset_index()\n",
    "\n",
    "# create binary indicator of whether naloxone received based on simple rule\n",
    "naloxone['binary_naloxone_admin'] = np.where((naloxone['x_doc_type'].str.contains('HED')) | \n",
    "                                            (naloxone['x_doc_type'].str.contains('EPIC IP ADMIN')),\n",
    "                                            1, 0)\n",
    "\n",
    "# attach to train/dev and validation sets\n",
    "df_train_dev = df_train_dev.merge(naloxone[['visit_occurrence_id', 'binary_naloxone_admin']], \n",
    "                                  how='left', on=['visit_occurrence_id'])\n",
    "df_valid = df_valid.merge(naloxone[['visit_occurrence_id', 'binary_naloxone_admin']], \n",
    "                          how='left', on=['visit_occurrence_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create numeric columns from string-based columns\n",
    "df_train_dev['binary_respiratory_failure_any'] = \\\n",
    "    np.where(df_train_dev['respiratory_failure_any'].str.contains('1'), 1, 0)\n",
    "df_valid['binary_respiratory_failure_any'] = \\\n",
    "    np.where(df_valid['respiratory_failure_any'].str.contains('1'), 1, 0)\n",
    "\n",
    "df_train_dev['binary_eligible_vent'] = \\\n",
    "    np.where(df_train_dev['eligible_vent'].str.contains('Yes'), 1, 0)\n",
    "df_valid['binary_eligible_vent'] = \\\n",
    "    np.where(df_valid['eligible_vent'].str.contains('Yes'), 1, 0)\n",
    "\n",
    "# coerce only categorical column into binary\n",
    "df_train_dev['binary_gender_female'] = np.where(df_train_dev['gender']=='FEMALE', 1, 0)\n",
    "df_valid['binary_gender_female'] = np.where(df_valid['gender']=='FEMALE', 1, 0)\n",
    "\n",
    "# replace missing values from naloxone join with \"0\"\n",
    "df_train_dev = df_train_dev.fillna(value={'binary_naloxone_admin': 0})\n",
    "df_valid = df_valid.fillna(value={'binary_naloxone_admin': 0})\n",
    "\n",
    "# replace NaN values with 0 for ICD conditions\n",
    "df_train_dev['binary_cond_resp_failure'] = np.where(df_train_dev['cond_resp_failure']==1, 1, 0)\n",
    "df_valid['binary_cond_resp_failure'] = np.where(df_valid['cond_resp_failure']==1, 1, 0)\n",
    "\n",
    "df_train_dev['binary_cond_sepsis'] = np.where(df_train_dev['cond_sepsis']==1, 1, 0)\n",
    "df_valid['binary_cond_sepsis'] = np.where(df_valid['cond_sepsis']==1, 1, 0)\n",
    "\n",
    "df_train_dev['binary_cond_cva'] = np.where(df_train_dev['cond_cva']==1, 1, 0)\n",
    "df_valid['binary_cond_cva'] = np.where(df_valid['cond_cva']==1, 1, 0)\n",
    "\n",
    "df_train_dev['binary_cond_resp_disease'] = np.where(df_train_dev['cond_resp_disease']==1, 1, 0)\n",
    "df_valid['binary_cond_resp_disease'] = np.where(df_valid['cond_resp_disease']==1, 1, 0)\n",
    "\n",
    "df_train_dev['binary_cond_cv_disease'] = np.where(df_train_dev['cond_cv_disease']==1, 1, 0)\n",
    "df_valid['binary_cond_cv_disease'] = np.where(df_valid['cond_cv_disease']==1, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Data Matrices  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify columns for model building\n",
    "cols_binary = df_train_dev.columns[df_train_dev.columns.str.contains('binary_')]\n",
    "cols_counts = df_train_dev.columns[df_train_dev.columns.str.contains('counts_')]\n",
    "\n",
    "cols = ['age_on_admission'] #'naloxone_admin_prob'\n",
    "cols.extend(cols_binary)\n",
    "cols.extend(cols_counts)\n",
    "#cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset columns\n",
    "X_train = df_train_dev[cols]\n",
    "X_valid = df_valid[cols]\n",
    "\n",
    "# also, some of the \"counts\" variables didn't have any results because those patients didn't have charts\n",
    "#    consider imputing \"0\" here, too\n",
    "X_train.fillna(0, inplace=True)\n",
    "X_valid.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale data \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_valid = sc.transform(X_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Discriminative Model(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attempt sample weights with by downweighting probabilities closer to 0.5\n",
    "weights = np.abs(df_train_dev['outcome_generative_model']-0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original best model (unchanged)\n",
    "best_rfcw = RandomForestClassifier(n_estimators=1000, random_state=SEED, \n",
    "                                   class_weight={0: 0.99, 1: 0.01}, max_depth=50, max_features=None)\n",
    "\n",
    "best_rfcw.fit(X_train, y_train_preds, sample_weight=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sensitivity 1 - unweighted\n",
    "rfc_unweighted = RandomForestClassifier(n_estimators=1000, random_state=SEED, \n",
    "                                    class_weight={0: 0.99, 1: 0.01}, max_depth=50, max_features=None)\n",
    "\n",
    "rfc_unweighted.fit(X_train, y_train_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't pull any information from manually-adjudicated Dev/Valid Sets\n",
    "y_train_preds_gen = np.where(df_train_dev['outcome_generative_model'] >= 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sensitivity 2 - Generative labels only (i.e., not Dev Set informed) & unweighted fit\n",
    "fully_generative_unweighted = RandomForestClassifier(n_estimators=1000, random_state=SEED, \n",
    "                                          class_weight={0: 0.99, 1: 0.01}, max_depth=50, max_features=None)\n",
    "\n",
    "fully_generative_unweighted.fit(X_train, y_train_preds_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sensitivity 3 - Generative labels only (i.e., not Dev Set informed) with weighted fit\n",
    "fully_generative_weighted = RandomForestClassifier(n_estimators=1000, random_state=SEED, \n",
    "                                                   class_weight={0: 0.99, 1: 0.01}, \n",
    "                                                   max_depth=50, max_features=None)\n",
    "\n",
    "weights = np.abs(df_train_dev['outcome_generative_model']-0.5)\n",
    "fully_generative_weighted.fit(X_train, y_train_preds_gen, sample_weight=weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance of Best Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Set Performance  \n",
    "\n",
    "FYI, the Generative model had an F1 score of 0.737 in the Validation Set.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifiers\n",
    "for model in [best_rfcw, rfc_unweighted, fully_generative_unweighted, fully_generative_weighted]:\n",
    "    y_pred = model.predict(X_valid)\n",
    "    print(model)\n",
    "    print(metrics.classification_report(Y_valid, y_pred, digits=3))\n",
    "    print(metrics.roc_auc_score(Y_valid, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From looking at the metrics here in the Validation Set, it was the sample weighting during the modeling fitting that influenced metrics & not the inclusion of manual adjudication information.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy of random guess in Validation set\n",
    "1-sum(Y_valid)/len(Y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probability distribution from weighted random forest classifier \n",
    "y_pred_proba = best_rfcw.predict_proba(X_valid)[:,CASE]\n",
    "\n",
    "eval = pd.DataFrame({'predicted': np.round(y_pred_proba, 2), 'actual': Y_valid})\n",
    "eval = eval.sort_values(by=['predicted', 'actual'])\n",
    "eval = eval.assign(counts =eval.groupby(['predicted']).cumcount())\n",
    "\n",
    "fig = sns.scatterplot(data=eval, x=\"predicted\", y=\"counts\", \n",
    "                      hue=eval[\"actual\"].tolist(), palette=\"colorblind\", s=100)\n",
    "plt.ylabel('Counts')\n",
    "plt.xlabel('Predicted Value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probability distribution from UNweighted random forest classifier \n",
    "y_pred_proba = rfc_unweighted.predict_proba(X_valid)[:,CASE]\n",
    "\n",
    "eval = pd.DataFrame({'predicted': np.round(y_pred_proba, 2), 'actual': Y_valid})\n",
    "eval = eval.sort_values(by=['predicted', 'actual'])\n",
    "eval = eval.assign(counts =eval.groupby(['predicted']).cumcount())\n",
    "\n",
    "fig = sns.scatterplot(data=eval, x=\"predicted\", y=\"counts\", \n",
    "                      hue=eval[\"actual\"].tolist(), palette=\"colorblind\", s=100)\n",
    "plt.ylabel('Counts')\n",
    "plt.xlabel('Predicted Value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-Fit \"Best\" Model on All Data (except Test Set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the train/dev set has an \"outcome_generative_model\" column that is used for creating\n",
    "#   weights in the weighted RF model - replicating that in the validation set before merging\n",
    "df_valid_temp = df_valid.copy()\n",
    "df_valid_temp['outcome_generative_model'] = y_valid_probs[:, CASE]\n",
    "\n",
    "# merge train/dev and validation sets \n",
    "df_final = df_train_dev.append(df_valid_temp, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update the 'outcome' column now that validation is also there\n",
    "df_final['outcome'] = np.where(pd.isnull(df_final['outcome']), # if label missing...\n",
    "                                  # pull from manual 'label' (same as above code)\n",
    "                                  np.where(df_final['label']=='case', upper_limit, lower_limit), \n",
    "                                  # otherwise, keep it what it is\n",
    "                                  df_final['outcome'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create y variables\n",
    "y_final_probs = np.array(df_final['outcome'])\n",
    "y_final_preds = np.where(df_final['outcome'] >= 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare features - code taken from above\n",
    "\n",
    "# subset columns\n",
    "X_final = df_final[cols]\n",
    "\n",
    "# also, some of the \"counts\" variables didn't have any results because those patients didn't have charts\n",
    "#    consider imputing \"0\" here, too\n",
    "X_final.fillna(0, inplace=True)\n",
    "\n",
    "# scale data \n",
    "X_final = sc.transform(X_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store weights\n",
    "weights_final = np.abs(df_final['outcome_generative_model']-0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_final = RandomForestClassifier(n_estimators=1000, random_state=SEED, \n",
    "                                     class_weight={0: 0.99, 1: 0.01}, max_depth=50, max_features=None)\n",
    "model_final.fit(X_final, y_final_preds, sample_weight=weights_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performance in training set (should be highly fit)\n",
    "y_pred = model_final.predict(X_final)\n",
    "y_pred_proba_final = model_final.predict_proba(X_final)[:,CASE]\n",
    "print(model_final)\n",
    "print(metrics.classification_report(y_final_preds, y_pred, digits=3))\n",
    "print(metrics.roc_auc_score(y_final_preds, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-fit the models from the sensitivity analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sensitivity 1 - unweighted\n",
    "rfc_unweighted = RandomForestClassifier(n_estimators=1000, random_state=SEED, \n",
    "                                    class_weight={0: 0.99, 1: 0.01}, max_depth=50, max_features=None)\n",
    "rfc_unweighted.fit(X_final, y_final_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't pull any information from manually-adjudicated Dev/Valid Sets\n",
    "y_final_preds_gen = np.where(df_final['outcome_generative_model'] >= 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sensitivity 2 - Generative labels only (i.e., not Dev Set informed) & unweighted fit\n",
    "fully_generative_unweighted = RandomForestClassifier(n_estimators=1000, random_state=SEED, \n",
    "                                          class_weight={0: 0.99, 1: 0.01}, max_depth=50, max_features=None)\n",
    "fully_generative_unweighted.fit(X_final, y_final_preds_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sensitivity 3 - Generative labels only (i.e., not Dev Set informed) with weighted fit\n",
    "fully_generative_weighted = RandomForestClassifier(n_estimators=1000, random_state=SEED, \n",
    "                                                   class_weight={0: 0.99, 1: 0.01}, \n",
    "                                                   max_depth=50, max_features=None)\n",
    "weights = np.abs(df_final['outcome_generative_model']-0.5)\n",
    "fully_generative_weighted.fit(X_final, y_final_preds_gen, sample_weight=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store predictions on data set & export for prediction model development \n",
    "df_final['snorkel_deterministic_model_prob'] = y_pred_proba_final\n",
    "df_final['sens_rfc_unweighted_prob'] = rfc_unweighted.predict_proba(X_final)[:,CASE]\n",
    "df_final['sens_fully_generative_unweighted_prob'] =fully_generative_unweighted.predict_proba(X_final)[:,CASE]\n",
    "df_final['sens_fully_generative_weighted_prob'] = fully_generative_weighted.predict_proba(X_final)[:,CASE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.to_csv('./train_dev_valid_set_with_predicted_labels.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply Predictions from Final Discriminative Model to Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeating code from above on train/dev and valid sets\n",
    "_, _, _, df_test = hlp.reattach_numeric_data(df_train_dev, df_train_dev, df_valid, df_test)\n",
    "\n",
    "df_test = df_test.merge(naloxone[['visit_occurrence_id', 'binary_naloxone_admin']], \n",
    "                          how='left', on=['visit_occurrence_id'])\n",
    "df_test['binary_respiratory_failure_any'] = \\\n",
    "    np.where(df_test['respiratory_failure_any'].str.contains('1'), 1, 0)\n",
    "df_test['binary_eligible_vent'] = \\\n",
    "    np.where(df_test['eligible_vent'].str.contains('Yes'), 1, 0)\n",
    "df_test['binary_gender_female'] = np.where(df_test['gender']=='FEMALE', 1, 0)\n",
    "df_test = df_test.fillna(value={'binary_naloxone_admin': 0})\n",
    "df_test['binary_cond_resp_failure'] = np.where(df_test['cond_resp_failure']==1, 1, 0)\n",
    "df_test['binary_cond_sepsis'] = np.where(df_test['cond_sepsis']==1, 1, 0)\n",
    "df_test['binary_cond_cva'] = np.where(df_test['cond_cva']==1, 1, 0)\n",
    "df_test['binary_cond_resp_disease'] = np.where(df_test['cond_resp_disease']==1, 1, 0)\n",
    "df_test['binary_cond_cv_disease'] = np.where(df_test['cond_cv_disease']==1, 1, 0)\n",
    "\n",
    "X_test = df_test[cols]\n",
    "X_test.fillna(0, inplace=True)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_binar_test = model_final.predict(X_test)\n",
    "y_pred_proba_test = model_final.predict_proba(X_test)[:,CASE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(y_pred_binar_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(y_pred_binar_test)/len(y_pred_binar_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(y_pred_proba_test, bins=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "non_small = y_pred_proba_test[np.where(y_pred_proba_test > 0.01)]\n",
    "plt.hist(non_small, bins=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep generative model probs for comparison\n",
    "df_test['snorkel_generative_model_prob'] = label_model.predict_proba(L_test)[:, CASE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "df_test['snorkel_deterministic_model_prob'] = y_pred_proba_test\n",
    "\n",
    "# sensitivity analyses\n",
    "df_test['sens_rfc_unweighted_prob'] = rfc_unweighted.predict_proba(X_test)[:,CASE]\n",
    "df_test['sens_fully_generative_unweighted_prob'] =fully_generative_unweighted.predict_proba(X_test)[:,CASE]\n",
    "df_test['sens_fully_generative_weighted_prob'] = fully_generative_weighted.predict_proba(X_test)[:,CASE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.to_csv('./test_set_with_predicted_labels.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
